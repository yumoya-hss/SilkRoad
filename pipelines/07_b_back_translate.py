import json
import sys
import os
import torch
import argparse
import gc
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# ==========================================
# é…ç½®åŒºåŸŸ
# ==========================================
NLLB_PATH = os.environ.get("SILKROAD_NLLB_MODEL","facebook/nllb-200-3.3B")

# NLLB è¯­è¨€ä»£ç æ˜ å°„
LANG_CODE_MAP = {
    "uyghur": "uig_Arab",
    "uzbek": "uzn_Latn",
    "kazakh": "kaz_Cyrl",
    "kyrgyz": "kir_Cyrl",
    "tajik": "tgk_Cyrl",
    "urdu": "urd_Arab",
    "chinese": "zho_Hans",
    "vietnamese": "vie_Latn",
    "mongolian": "mon_Cyrl",
    "bengali": "ben_Beng",
    "pashto": "pbt_Arab"
}

TARGET_LANG_CODE = "eng_Latn"

def load_data(file_path):
    print(f"ğŸ“– è¯»å–æ•°æ®: {file_path} ...")
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            if f.read(1) == '[':
                f.seek(0); return json.load(f)
            else:
                f.seek(0); return [json.loads(line) for line in f if line.strip()]
    except Exception as e:
        print(f"âŒ æ•°æ®é”™è¯¯: {e}"); sys.exit(1)

def batch_translate_smart(model, tokenizer, task_items, src_lang_code, device, batch_size):
    """
    æ‰§è¡Œæ™ºèƒ½æ‰¹é‡å›è¯‘ï¼ˆæŒ‰é•¿åº¦æ’åºï¼‰
    task_items: [(data_idx, field_key, text), ...]
    Return: List of (data_idx, field_key, translated_text)
    """
    # 1. [Smart Batching] æŒ‰æ–‡æœ¬é•¿åº¦å€’åºæ’åºï¼Œå‡å°‘ Padding æµªè´¹ 
    # å€’åºé€šå¸¸æ¯”æ­£åºç¨å¥½ï¼Œå› ä¸ºæœ€é•¿çš„å…ˆå¤„ç†ï¼Œé˜²æ­¢æœ€åæ˜¾å­˜ç¢ç‰‡
    sorted_tasks = sorted(task_items, key=lambda x: len(x[2]), reverse=True)
    
    results = []
    
    # è®¾ç½®æºè¯­è¨€ (è¿™å¯¹ NLLB è‡³å…³é‡è¦)
    tokenizer.src_lang = src_lang_code
    forced_bos_id = tokenizer.convert_tokens_to_ids(TARGET_LANG_CODE)

    # 2. æ‰¹é‡æ¨ç†
    for i in tqdm(range(0, len(sorted_tasks), batch_size), desc=f"   Processing {src_lang_code}"):
        batch = sorted_tasks[i : i + batch_size]
        batch_texts = [item[2] for item in batch] # æå–æ–‡æœ¬
        
        # ç¼–ç 
        inputs = tokenizer(
            batch_texts, 
            return_tensors="pt", 
            padding=True, 
            truncation=True, 
            max_length=128
        ).to(device)
        
        # ç”Ÿæˆ (å›è¯‘è¿½æ±‚è¯­ä¹‰è¿˜åŸï¼Œè´ªå©ªæœç´¢ num_beams=1 æœ€å¿«ä¸”æœ€å¿ å®åŸæ–‡)
        with torch.inference_mode():
            generated_tokens = model.generate(
                **inputs,
                forced_bos_token_id=forced_bos_id,
                max_new_tokens=128,
                num_beams=1, 
                do_sample=False
            )
        
        decoded = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
        
        # ç»“æœä¸åŸå§‹å…ƒæ•°æ®é‡æ–°ç»‘å®š
        for j, trans_text in enumerate(decoded):
            original_meta = batch[j] # (data_idx, field_key, original_text)
            results.append((original_meta[0], original_meta[1], trans_text.strip()))
            
    return results

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_file", type=str, required=True)
    parser.add_argument("--output_file", type=str, required=True)
    parser.add_argument("--batch_size", type=int, default=64) # æ’åºåå¯ä»¥å°è¯•æ›´å¤§çš„ batch
    parser.add_argument("--gpu_id", type=int, default=0)
    args = parser.parse_args()

    device = f"cuda:{args.gpu_id}" if torch.cuda.is_available() else "cpu"
    
    # 1. åŠ è½½æ•°æ®
    data = load_data(args.input_file)
    
    # 2. åŠ è½½æ¨¡å‹ (FP16 + Flash Attention)
    print(f"\nğŸš€ åŠ è½½ NLLB æ¨¡å‹ (FP16)...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(NLLB_PATH)
        model = AutoModelForSeq2SeqLM.from_pretrained(
            NLLB_PATH, 
            torch_dtype=torch.float16,
            attn_implementation="flash_attention_2" if torch.cuda.get_device_capability()[0] >= 8 else "eager"
        ).to(device).eval()
    except:
        print("âš ï¸ Flash Attention ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤åŠ è½½...")
        model = AutoModelForSeq2SeqLM.from_pretrained(NLLB_PATH, torch_dtype=torch.float16).to(device).eval()

    # 3. æ”¶é›†ä»»åŠ¡ (æŒ‰è¯­è¨€åˆ†ç»„)
    lang_tasks = {} # { "uyghur": [ (idx, key, text), ... ] }
    
    print("ğŸ” æ•´ç†ä»»åŠ¡é˜Ÿåˆ—...")
    count = 0
    for idx, item in enumerate(data):
        if 'translations' not in item: continue
        for lang, trans_dict in item['translations'].items():
            if lang not in LANG_CODE_MAP: continue
            
            if lang not in lang_tasks: lang_tasks[lang] = []
            
            for key, text in trans_dict.items():
                # è·³è¿‡å›è¯‘(bt_)ã€åˆ†æ•°(score_)å’Œç©ºå€¼
                if not key.startswith('bt_') and not key.startswith('score_') and text and text.strip():
                    lang_tasks[lang].append((idx, key, text))
                    count += 1
    
    print(f"âœ… æ€»å›è¯‘ä»»åŠ¡æ•°: {count}")

    # 4. æ‰§è¡Œå›è¯‘ (Smart Batching Pipeline)
    for lang, tasks in lang_tasks.items():
        src_code = LANG_CODE_MAP[lang]
        print(f"\nğŸŒ æ­£åœ¨å›è¯‘: {lang} -> English (Tasks: {len(tasks)})")
        
        # è¿™ä¸€æ­¥ä¼šè‡ªåŠ¨æ’åºã€æ‰¹é‡ç¿»è¯‘
        results = batch_translate_smart(model, tokenizer, tasks, src_code, device, args.batch_size)
        
        # å›å¡«æ•°æ® (ç”±äºæˆ‘ä»¬æºå¸¦äº† idx å’Œ keyï¼Œæ‰€ä»¥ä¹±åºå¤„ç†ä¹Ÿèƒ½ç²¾å‡†å›å¡«)
        print(f"   Writing results to memory...")
        for data_idx, key, bt_text in results:
            bt_key = f"bt_{key}"
            data[data_idx]['translations'][lang][bt_key] = bt_text
            
        # æ˜¾å­˜æ•´ç† (æ¯ä¸ªè¯­è¨€è·‘å®Œæ¸…ç†ä¸€æ¬¡ï¼Œä¿æŒçŠ¶æ€æœ€ä½³)
        torch.cuda.empty_cache()

    # 5. ä¿å­˜
    print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆç»“æœè‡³: {args.output_file}")
    with open(args.output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
        
    print("ğŸ‰ å›è¯‘å…¨éƒ¨å®Œæˆï¼")

if __name__ == "__main__":
    main()