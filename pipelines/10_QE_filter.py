import os
import json
import sys
import argparse
from collections import defaultdict

# ==========================================
# ğŸ”¥ [ä¸¥é€‰é˜ˆå€¼é…ç½®] ğŸ”¥
# ==========================================
THRESHOLD_BERT = 0.90   
THRESHOLD_COMET = 0.78  
THRESHOLD_CLIP = 0.27   

# å®šä¹‰æ‰€æœ‰å‚èµ›æ¨¡å‹ (ç”¨äºéç»´å¾å°”è¯­)
MODELS = ["nllb", "seamless", "qwen", "madlad"]

def load_data(file_path):
    print(f"ğŸ“– è¯»å–æ•°æ®: {file_path} ...")
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            first_char = f.read(1)
            f.seek(0)
            if first_char == '[':
                return json.load(f)
            else:
                return [json.loads(line) for line in f if line.strip()]
    except Exception as e:
        print(f"âŒ è¯»å–å¤±è´¥: {e}"); sys.exit(1)

def select_best_candidate(trans_obj, type_prefix, lang_code):
    """
    è¾“å…¥: trans_obj, type_prefix, lang_code
    è¾“å‡º: (best_text, best_model_name, best_scores) or (None, None, None)
    """
    candidates = []
    
    # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šé’ˆå¯¹ç»´å¾å°”è¯­çš„ç‰¹æ®Šé€»è¾‘
    # å¦‚æœæ˜¯ç»´å¾å°”è¯­ (ug / uyghur)ï¼Œåªè€ƒè™‘ NLLB
    if lang_code in ['ug', 'uyghur']:
        target_models = ['nllb']
    else:
        # å…¶ä»–è¯­è¨€ï¼Œå››ä¸ªæ¨¡å‹ä¸€èµ·ç«äº‰
        target_models = MODELS

    # 1. éå†ç›®æ ‡æ¨¡å‹ï¼Œæ”¶é›†å‚èµ›é€‰æ‰‹
    for model in target_models:
        text_key = f"{type_prefix}_{model}"
        
        # æ£€æŸ¥è¯¥æ¨¡å‹æ˜¯å¦æœ‰ç¿»è¯‘æ–‡æœ¬
        if text_key in trans_obj and trans_obj[text_key]:
            # è·å–åˆ†æ•° (å¦‚æœç¼ºå¤±åˆ™ç»™ -1)
            bert = trans_obj.get(f"score_bert_{type_prefix}_{model}", -1)
            comet = trans_obj.get(f"score_comet_{type_prefix}_{model}", -1)
            visual = trans_obj.get(f"score_visual_{type_prefix}_{model}", -1)
            
            candidates.append({
                "model": model,
                "text": trans_obj[text_key],
                "bert": bert,
                "comet": comet,
                "visual": visual
            })

    # 2. èµ„æ ¼èµ› (Hard Filtering)
    qualified = []
    for cand in candidates:
        # å¿…é¡»åŒæ—¶æ»¡è¶³ä¸‰ä¸ªç¡¬æŒ‡æ ‡
        if (cand['bert'] >= THRESHOLD_BERT and 
            cand['comet'] >= THRESHOLD_COMET and 
            cand['visual'] >= THRESHOLD_CLIP):
            qualified.append(cand)

    if not qualified:
        return None, None, None

    # 3. å†³èµ› (Winner Takes All)
    # æŒ‰ COMET åˆ†æ•°ä»é«˜åˆ°ä½æ’åºï¼Œå–ç¬¬ä¸€å
    # å¯¹äºç»´å¾å°”è¯­ï¼Œå› ä¸ºåªæœ‰ NLLB ä¸€ä¸ªå€™é€‰ï¼Œæ‰€ä»¥åªè¦ qualified åˆ—è¡¨ä¸ä¸ºç©ºï¼Œå–å‡ºæ¥çš„å°±æ˜¯ NLLB
    best_cand = sorted(qualified, key=lambda x: x['comet'], reverse=True)[0]
    
    return best_cand['text'], best_cand['model'], {
        "bert": best_cand['bert'],
        "comet": best_cand['comet'],
        "visual": best_cand['visual']
    }

def process_dataset(data, output_file):
    print("ğŸš€ å¼€å§‹æ‰§è¡Œç­›é€‰ç­–ç•¥...")
    print(f"   - ç»´å¾å°”è¯­ (Uyghur): ä»… NLLB ç‹¬å®¶é€šé“")
    print(f"   - å…¶ä»–è¯­è¨€: {MODELS} å››æ¨¡å‹ç«æŠ€")
    
    final_data = []
    stats = {
        "total": len(data),
        "kept": 0,
        "lang_stats": defaultdict(lambda: defaultdict(int))
    }

    for item in data:
        if 'translations' not in item: continue
        
        new_translations = {}
        has_content = False
        
        # éå†æ¯ç§è¯­è¨€
        for lang, trans_obj in item['translations'].items():
            lang_key = lang.lower()
            lang_result = {}
            
            # ä¼ å…¥ lang_key ä»¥ä¾¿åŒºåˆ†ç­–ç•¥
            
            # --- å¤„ç† Short Caption ---
            s_text, s_model, s_scores = select_best_candidate(trans_obj, "short", lang_key)
            if s_text:
                lang_result["short_translation"] = s_text
                lang_result["short_model"] = s_model
                lang_result["short_scores"] = s_scores
                stats["lang_stats"][lang_key][f"short_{s_model}"] += 1
            
            # --- å¤„ç† Long Caption ---
            l_text, l_model, l_scores = select_best_candidate(trans_obj, "long", lang_key)
            if l_text:
                lang_result["long_translation"] = l_text
                lang_result["long_model"] = l_model
                lang_result["long_scores"] = l_scores
                stats["lang_stats"][lang_key][f"long_{l_model}"] += 1
            
            if lang_result:
                new_translations[lang] = lang_result
                has_content = True

        if has_content:
            final_item = {
                "image_id": item['image_id'],
                "path": item['path'],
                "src_short": item.get('short_caption_best'),
                "src_long": item.get('long_caption_best'),
                "translations": new_translations
            }
            final_data.append(final_item)
            stats["kept"] += 1

    # ä¿å­˜
    print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆé»„é‡‘æ•°æ®é›†è‡³: {output_file}")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)

    # æ‰“å°æˆ˜æŠ¥
    print("\n" + "="*80)
    print("ğŸ† FINAL DATASET STATISTICS")
    print("="*80)
    print(f"åŸå§‹æ•°æ® : {stats['total']}")
    print(f"æœ€ç»ˆä¿ç•™ : {stats['kept']} (ä¿ç•™ç‡: {stats['kept']/stats['total']*100:.2f}%)")
    print("-" * 80)
    
    # åŠ¨æ€ç”Ÿæˆè¡¨å¤´
    headers = ["Language"]
    for m in MODELS:
        headers.append(f"S-{m[:2].upper()}") 
        headers.append(f"L-{m[:2].upper()}")
    
    header_str = "{:<12} | " + " ".join([f"{{:<6}}" for _ in range(len(headers)-1)])
    print(header_str.format(*headers))
    print("-" * 80)
    
    for lang in sorted(stats["lang_stats"].keys()):
        counts = stats["lang_stats"][lang]
        row_vals = [lang]
        for m in MODELS:
            row_vals.append(counts[f"short_{m}"])
            row_vals.append(counts[f"long_{m}"])
        
        print(header_str.format(*row_vals))
        
    print("="*80)
    print("âœ¨ ç­›é€‰å®Œæˆã€‚")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_file", type=str, default=os.environ.get("SILKROAD_SCORED","outputs/scored/scored.json"))
    parser.add_argument("--output_file", type=str, default=os.environ.get("SILKROAD_GOLDEN","outputs/final/golden.json"))
    args = parser.parse_args()

    data = load_data(args.input_file)
    process_dataset(data, args.output_file)