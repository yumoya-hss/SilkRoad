import json
import os
import random

# ================= ğŸ”´ é…ç½®åŒºåŸŸ ğŸ”´ =================
# 1. Split åçš„æ ¹ç›®å½• (é‡Œé¢åº”è¯¥æœ‰ train, val, test ä¸‰ä¸ªæ–‡ä»¶å¤¹)
SPLIT_ROOT = "/home/houshuoshuo/qlora_data/split"

# 2. å›¾ç‰‡æ ¹ç›®å½• (ImageNet50K)
IMAGE_ROOT = "/mnt/raid/hss/dataset/Image50K"

# 3. è¾“å‡ºæ–‡ä»¶ä¿å­˜ç›®å½• (å»ºè®®å°±ä¿å­˜åœ¨ split æ ¹ç›®å½•ä¸‹ï¼Œæ–¹ä¾¿ç®¡ç†)
OUTPUT_DIR = "/home/houshuoshuo/qlora_data/split"

# éšæœºç§å­ (ä¿è¯æ‰“ä¹±é¡ºåºä¸€è‡´)
SEED = 42
# ===================================================

def convert_single_split(split_name):
    """
    å¤„ç†å•ä¸ªåˆ’åˆ† (å¦‚ 'train', 'val', 'test')
    """
    input_dir = os.path.join(SPLIT_ROOT, split_name)
    output_file = os.path.join(OUTPUT_DIR, f"silkroad_{split_name}.json")
    
    all_data = []

    # è¯­è¨€ä»£ç æ˜ å°„è¡¨
    lang_map = {
        'ug': 'Uyghur', 'uyghur': 'Uyghur',
        'uz': 'Uzbek', 'uzbek': 'Uzbek',
        'kk': 'Kazakh', 'kazakh': 'Kazakh',
        'ur': 'Urdu', 'urdu': 'Urdu',
        'ky': 'Kyrgyz', 'kyrgyz': 'Kyrgyz',
        'tg': 'Tajik', 'tajik': 'Tajik'
    }

    if not os.path.exists(input_dir):
        print(f"âŒ Error: æ‰¾ä¸åˆ°ç›®å½• {input_dir}")
        return

    files = [f for f in os.listdir(input_dir) if f.endswith('.json')]
    # æ’åºä»¥ä¿è¯å¤„ç†é¡ºåºä¸€è‡´
    files.sort()
    
    print(f"ğŸš€ æ­£åœ¨å¤„ç† [{split_name}] é›†: æ‰«æåˆ° {len(files)} ä¸ªæ–‡ä»¶...")

    for filename in files:
        # ä¿®æ­£æ–‡ä»¶åè§£æé€»è¾‘ï¼š
        # ä¸Šä¸€æ­¥åˆ‡åˆ†ç”Ÿæˆçš„å¯èƒ½æ˜¯ "kazakh.json"ï¼Œæ²¡æœ‰ä¸‹åˆ’çº¿äº†
        # æ‰€ä»¥ç›´æ¥å»æ‰ .json åç¼€å³å¯æ‹¿åˆ°è¯­è¨€å
        lang_key = filename.replace('.json', '').lower()
        
        # å…¼å®¹é€»è¾‘ï¼šä¸‡ä¸€æ–‡ä»¶åé‡Œè¿˜æœ‰ä¸‹åˆ’çº¿ (å¦‚ dataset_kazakh.json)ï¼Œå°è¯•æå–
        if '_' in lang_key:
             # è¿™é‡Œå‡è®¾è¯­è¨€ååœ¨æœ€åï¼Œæˆ–è€…ä½ è‡ªå·±æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
             # æ¯”å¦‚ dataset_kazakh -> kazakh
             parts = lang_key.split('_')
             # ç®€å•çš„å¯å‘å¼ï¼šçœ‹å“ªéƒ¨åˆ†åœ¨æ˜ å°„è¡¨é‡Œ
             found = False
             for p in parts:
                 if p in lang_map:
                     lang_key = p
                     found = True
                     break
             if not found:
                 lang_key = parts[0] # é»˜è®¤å–ç¬¬ä¸€éƒ¨åˆ†

        target_lang = lang_map.get(lang_key, "Target Language")

        file_path = os.path.join(input_dir, filename)
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # print(f"   -> è¯»å– {filename} ({target_lang}): {len(data)} æ¡")

        for item in data:
            # æ„é€ ç»å¯¹å›¾ç‰‡è·¯å¾„
            img_filename = os.path.basename(item.get('path', ''))
            abs_img_path = os.path.join(IMAGE_ROOT, img_filename)

            source_text = item.get('src_text', '')
            target_text = item.get('tgt_text', '')

            if not source_text or not target_text: continue

            # æ„é€  Prompt
            conversation = [
                {
                    "from": "human",
                    "value": f"<image>\nPlease translate the description of this image into {target_lang}.\nEnglish Source: {source_text}"
                },
                {
                    "from": "gpt",
                    "value": target_text
                }
            ]

            all_data.append({
                "images": [abs_img_path],
                "conversations": conversation
            })

    # æ‰“ä¹±æ•°æ®
    random.seed(SEED)
    random.shuffle(all_data)

    print(f"âœ… [{split_name}] åˆå¹¶å®Œæˆï¼æ€»å…± {len(all_data)} æ¡æ ·æœ¬ã€‚")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ å·²ä¿å­˜: {output_file}\n")

if __name__ == "__main__":
    # æ‰¹é‡å¤„ç†ä¸‰ä¸ªæ–‡ä»¶å¤¹
    for split in ['train', 'val', 'test']:
        convert_single_split(split)
