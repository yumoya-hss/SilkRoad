import os
import json
import shutil
import torch
import math
import sacrebleu
import pandas as pd
from PIL import Image
from tqdm import tqdm
import yaml
import logging

# ================= ğŸ”´ 1. å¼ºåˆ¶ç¦»çº¿ç¯å¢ƒè®¾ç½® ğŸ”´ =================
os.environ["HF_DATASETS_OFFLINE"] = "1"
os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
logging.getLogger("transformers").setLevel(logging.ERROR)

# ================= ğŸ”´ 2. è·¯å¾„é…ç½® ğŸ”´ =================
BASELINE_FILE = "pred_baseline.json"
FINETUNED_FILE = "pred_finetuned_v2_3epoch.json"
OUTPUT_CSV = "final_experiment_results_full.csv"

# 1. CometKiwi
KIWI_ROOT = "/mnt/raid/hss/model/wmt22-cometkiwi-da"
INFOXLM_PATH = "/mnt/raid/hss/model/infoxlm-large"

# 2. BERTScore (XLM-R)
XLMR_PATH = "/mnt/raid/hss/model/xlm-roberta-large"

# 3. âœ… PPL (æ”¹ä¸º Multilingual BERT)
# å‡è®¾æ‚¨çš„ mBERT è·¯å¾„å¦‚ä¸‹ï¼Œå¦‚æœæ²¡æœ‰ï¼Œå¯æ”¹ä¸º XLMR_PATH (XLM-R ä¹Ÿæ˜¯ BERT æ¶æ„)
PPL_MODEL_PATH = "/mnt/raid/hss/model/xlm-roberta-large"

# 4. Visual Models
SIGLIP_PATH = "/mnt/raid/hss/model/siglip-so400m-patch14-384"
CLIP_BASE_PATH = "/mnt/raid/hss/model/clip-vit-base-patch32"
CLIP_LARGE_PATH = "/mnt/raid/hss/model/clip-vit-large-patch14"
# =========================================================

# -----------------------------------------------------------------------------
# ğŸ› ï¸ æ­¥éª¤ 0: ç‰©ç†ä¿®å¤
# -----------------------------------------------------------------------------
def check_and_fix_files():
    print("ğŸ”§ [Step 0] Checking files...")
    
    # 1. InfoXLM
    if not os.path.exists(INFOXLM_PATH): os.makedirs(INFOXLM_PATH, exist_ok=True)
    sp_path = os.path.join(INFOXLM_PATH, "sentencepiece.bpe.model")
    if not os.path.exists(sp_path):
        src = os.path.join(XLMR_PATH, "sentencepiece.bpe.model")
        if os.path.exists(src): shutil.copy(src, sp_path)
    
    tok_conf = os.path.join(INFOXLM_PATH, "tokenizer_config.json")
    with open(tok_conf, 'w') as f:
        json.dump({"do_lower_case": False, "unk_token": "<unk>", "sep_token": "</s>", 
                   "pad_token": "<pad>", "cls_token": "<s>", "mask_token": "<mask>", 
                   "model_type": "xlm-roberta", "use_fast": False}, f)

    # 2. Kiwi hparams
    yaml_path = os.path.join(KIWI_ROOT, "hparams.yaml")
    if not os.path.exists(yaml_path): yaml_path = os.path.join(KIWI_ROOT, "checkpoints", "hparams.yaml")
    if os.path.exists(yaml_path):
        with open(yaml_path, 'r') as f: config = yaml.safe_load(f) or {}
        if config.get("encoder_model") != INFOXLM_PATH:
            config["encoder_model"] = INFOXLM_PATH
            config["pretrained_model"] = INFOXLM_PATH
            config["load_weights_from_checkpoint"] = True
            with open(yaml_path, 'w') as f: yaml.dump(config, f)

    # 3. CLIP Config
    for clip_path in [CLIP_BASE_PATH, CLIP_LARGE_PATH]:
        if os.path.exists(clip_path):
            prep_conf = os.path.join(clip_path, "preprocessor_config.json")
            if not os.path.exists(prep_conf):
                dummy_prep = {
                    "crop_size": 224, "do_center_crop": True, "do_convert_rgb": True, "do_normalize": True,
                    "do_resize": True, "feature_extractor_type": "CLIPFeatureExtractor",
                    "image_mean": [0.48145466, 0.4578275, 0.40821073], "image_std": [0.26862954, 0.26130258, 0.27577711],
                    "resample": 3, "size": 224
                }
                try: 
                    with open(prep_conf, 'w') as f: json.dump(dummy_prep, f)
                except: 
                    pass

# -----------------------------------------------------------------------------
# ğŸ§™â€â™‚ï¸ æ­¥éª¤ 1: æ™ºèƒ½è·¯ç”±æ‹¦æˆªå™¨
# -----------------------------------------------------------------------------
from transformers import (
    AutoTokenizer, AutoModel, AutoConfig, 
    AutoModelForCausalLM, AutoModelForMaskedLM, # âœ… å¼•å…¥ MaskedLM
    XLMRobertaTokenizer, XLMRobertaTokenizerFast, XLMRobertaModel, XLMRobertaConfig,
    CLIPModel, CLIPProcessor, SiglipModel, SiglipProcessor
)

GLOBAL_TOKENIZERS = {}

def install_smart_router():
    print("ğŸ§™â€â™‚ï¸ [Step 1] Installing Smart Router Interceptor...")
    try:
        GLOBAL_TOKENIZERS['infoxlm'] = XLMRobertaTokenizer(vocab_file=os.path.join(INFOXLM_PATH, "sentencepiece.bpe.model"))
    except: pass
    
    def router_interceptor(original_func, cls_name):
        def wrapper(cls, pretrained_model_name_or_path, *args, **kwargs):
            path_str = str(pretrained_model_name_or_path)
            
            # è·¯ç”±é€»è¾‘
            if "infoxlm" in path_str.lower() or path_str == INFOXLM_PATH:
                if "Tokenizer" in cls_name and 'infoxlm' in GLOBAL_TOKENIZERS: return GLOBAL_TOKENIZERS['infoxlm']
                if not os.path.exists(path_str):
                    pretrained_model_name_or_path = INFOXLM_PATH
                    kwargs['local_files_only'] = True
            elif "xlm-roberta" in path_str.lower() and "infoxlm" not in path_str.lower():
                if not os.path.exists(path_str) or "huggingface.co" in path_str:
                    pretrained_model_name_or_path = XLMR_PATH
                    kwargs['local_files_only'] = True
            
            # âœ… æ–°å¢: PPL (mBERT)
            elif "bert-" in path_str.lower() or "multilingual" in path_str.lower():
                if not os.path.exists(path_str):
                    pretrained_model_name_or_path = PPL_MODEL_PATH
                    kwargs['local_files_only'] = True

            elif "clip" in path_str.lower() and "siglip" not in path_str.lower():
                if "base" in path_str.lower() and not os.path.exists(path_str):
                    pretrained_model_name_or_path = CLIP_BASE_PATH
                    kwargs['local_files_only'] = True
                elif "large" in path_str.lower() and not os.path.exists(path_str):
                    pretrained_model_name_or_path = CLIP_LARGE_PATH
                    kwargs['local_files_only'] = True
            
            return original_func(cls, pretrained_model_name_or_path, *args, **kwargs)
        return wrapper

    target_classes = [AutoTokenizer, AutoModel, AutoConfig, AutoModelForCausalLM, AutoModelForMaskedLM, XLMRobertaTokenizer, XLMRobertaTokenizerFast, XLMRobertaConfig, XLMRobertaModel, CLIPModel, CLIPProcessor]
    for cls in target_classes:
        if hasattr(cls, 'from_pretrained'):
            cls.from_pretrained = classmethod(router_interceptor(cls.from_pretrained.__func__, cls.__name__))

    print("   âœ… Smart Router Active.")

check_and_fix_files()
install_smart_router()

# =============================================================================
# ğŸš€ æ­¥éª¤ 2: é€šç”¨æ‰‹åŠ¨åŠ è½½å‡½æ•°
# =============================================================================
def load_manual_model(model_path, model_class, device, dtype=None):
    print(f"   -> Manual Load: {os.path.basename(model_path)} ({model_class.__name__})")
    try:
        config = AutoConfig.from_pretrained(model_path, trust_remote_code=True, local_files_only=True)
        
        # åˆå§‹åŒ–æ¨¡å‹
        if hasattr(model_class, 'from_config'):
            model = model_class.from_config(config)
        else:
            model = model_class(config)
            
        if dtype: model = model.to(dtype)
            
        # å¯»æ‰¾æƒé‡
        safe_file = os.path.join(model_path, "model.safetensors")
        safe_index = os.path.join(model_path, "model.safetensors.index.json")
        bin_file = os.path.join(model_path, "pytorch_model.bin")
        bin_index = os.path.join(model_path, "pytorch_model.bin.index.json")

        if os.path.exists(safe_file) or os.path.exists(safe_index):
            print("      ğŸ›¡ï¸ Safetensors detected...")
            model = model_class.from_pretrained(model_path, config=config, trust_remote_code=True, local_files_only=True, torch_dtype=dtype)
        elif os.path.exists(bin_file):
            print("      ğŸ›¡ï¸ Pickle (.bin) detected. Using raw torch.load...")
            state_dict = torch.load(bin_file, map_location="cpu")
            model.load_state_dict(state_dict, strict=False)
        elif os.path.exists(bin_index):
            print("      âš ï¸ Sharded .bin detected. Attempting standard load...")
            model = model_class.from_pretrained(model_path, config=config, trust_remote_code=True, local_files_only=True, torch_dtype=dtype)
        else:
            raise FileNotFoundError("No model weights found.")

        return model.to(device).eval()

    except Exception as e:
        print(f"      âŒ Manual Load Failed: {e}")
        try:
            return model_class.from_pretrained(model_path, local_files_only=True, trust_remote_code=True).to(device).eval()
        except:
            return None

# =============================================================================
# ğŸš€ æ­¥éª¤ 3: åŠ è½½æ¨¡å‹
# =============================================================================
from comet import load_from_checkpoint
try:
    from bert_score import score as run_bert_score
    HAS_BERTSCORE = True
except ImportError:
    HAS_BERTSCORE = False

def load_models():
    print("\nâ³ [Step 2] Loading Evaluation Models...")
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # 1. CometKiwi
    kiwi_model = None
    try:
        ckpt_path = os.path.join(KIWI_ROOT, "checkpoints", "model.ckpt")
        if not os.path.exists(ckpt_path): ckpt_path = os.path.join(KIWI_ROOT, "model.ckpt")
        print(f"   -> Loading CometKiwi from: {ckpt_path}")
        kiwi_model = load_from_checkpoint(ckpt_path, strict=False)
        print("      âœ… CometKiwi Loaded!")
    except Exception as e:
        print(f"      âŒ CometKiwi Failed: {e}")

    # 2. âœ… PPL (mBERT) - AutoModelForMaskedLM
    ppl_bundle = None
    try:
        if os.path.exists(PPL_MODEL_PATH):
            tok = AutoTokenizer.from_pretrained(PPL_MODEL_PATH, trust_remote_code=True, use_fast=False)
            # ä½¿ç”¨ AutoModelForMaskedLM
            mod = load_manual_model(PPL_MODEL_PATH, AutoModelForMaskedLM, device)
            if mod:
                ppl_bundle = (mod, tok)
                print("      âœ… Multilingual BERT Loaded!")
        else:
            print(f"      âŒ mBERT path not found: {PPL_MODEL_PATH}")
    except Exception as e:
        print(f"      âŒ mBERT Failed: {e}")

    # 3. Visual Models
    visual_bundle = {}
    try:
        if os.path.exists(SIGLIP_PATH):
            visual_bundle['siglip'] = (
                SiglipModel.from_pretrained(SIGLIP_PATH, local_files_only=True).to(device).eval(),
                SiglipProcessor.from_pretrained(SIGLIP_PATH, local_files_only=True)
            )
            print("      âœ… SigLIP Loaded!")
    except Exception as e: print(f"      âŒ SigLIP Failed: {e}")

    try:
        if os.path.exists(CLIP_BASE_PATH):
            mod = load_manual_model(CLIP_BASE_PATH, CLIPModel, device)
            proc = CLIPProcessor.from_pretrained(CLIP_BASE_PATH, local_files_only=True)
            if mod:
                visual_bundle['clip_base'] = (mod, proc)
                print("      âœ… CLIP-Base Loaded!")
    except Exception as e: print(f"      âŒ CLIP-Base Failed: {e}")

    try:
        if os.path.exists(CLIP_LARGE_PATH):
            mod = load_manual_model(CLIP_LARGE_PATH, CLIPModel, device)
            proc = CLIPProcessor.from_pretrained(CLIP_LARGE_PATH, local_files_only=True)
            if mod:
                visual_bundle['clip_large'] = (mod, proc)
                print("      âœ… CLIP-Large Loaded!")
    except Exception as e: print(f"      âŒ CLIP-Large Failed: {e}")

    return kiwi_model, ppl_bundle, visual_bundle

# =============================================================================
# ğŸ“Š æ­¥éª¤ 4: è®¡ç®—é€»è¾‘ (ğŸ”¥ é‡å†™ PPL è®¡ç®—ä¸º Pseudo-Perplexity)
# =============================================================================
def calculate_pseudo_ppl(texts, model, tokenizer):
    """
    è®¡ç®— MLM (å¦‚ BERT) çš„ä¼ªå›°æƒ‘åº¦ (Pseudo-Perplexity)
    """
    if not texts: return 0.0
    device = model.device
    mask_id = tokenizer.mask_token_id
    if mask_id is None: return 0.0 # æ— æ³•è®¡ç®—

    nlls = []
    
    # é€å¥è®¡ç®—ï¼ˆæˆ–è€…å° Batch è®¡ç®—ï¼‰
    for text in texts:
        if len(text.strip()) == 0: continue
        
        # 1. ç¼–ç 
        inputs = tokenizer(text, return_tensors="pt", padding=False, truncation=True, max_length=512).to(device)
        input_ids = inputs["input_ids"] # shape: [1, seq_len]
        seq_len = input_ids.shape[1]
        
        # å¿½ç•¥å¤ªçŸ­çš„å¥å­
        if seq_len < 3: continue 

        # 2. æ„é€  Batch Masking
        # æˆ‘ä»¬è¦è®¡ç®— P(token_i | other_tokens)ï¼Œæ‰€ä»¥è¦æ„å»º seq_len ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬ mask æ‰ç¬¬ i ä¸ªè¯
        # ä¸ºäº†é€Ÿåº¦ï¼Œæˆ‘ä»¬è·³è¿‡ CLS(0) å’Œ SEP(-1)
        
        # å¤åˆ¶ seq_len-2 ä»½
        repeat_ids = input_ids.repeat(seq_len-2, 1) # [seq_len-2, seq_len]
        
        # åˆ›å»ºå¯¹è§’çº¿ mask çŸ©é˜µ
        # mask çš„ä½ç½®ç´¢å¼•æ˜¯ 1 åˆ° seq_len-2
        for i in range(seq_len-2):
            repeat_ids[i, i+1] = mask_id
            
        # 3. æ¨ç† (Batch Inference)
        # æ³¨æ„æ˜¾å­˜ï¼Œå¦‚æœå¥å­å¾ˆé•¿ï¼Œrepeat_ids ä¼šå¾ˆå¤§ï¼Œéœ€è¦åˆ†å—
        # è¿™é‡Œåšä¸€ä¸ªç®€å•çš„æ˜¾å­˜ä¿æŠ¤ï¼šå¦‚æœ batch > 64ï¼Œåˆ†å—å¤„ç†
        batch_size = 64
        total_loss = 0.0
        
        for i in range(0, repeat_ids.shape[0], batch_size):
            chunk_ids = repeat_ids[i:i+batch_size]
            
            with torch.no_grad():
                outputs = model(chunk_ids)
                logits = outputs.logits # [batch, seq_len, vocab_size]
            
            # 4. æå–è¢« Mask ä½ç½®çš„ Logits
            # chunk_ids ä¸­ï¼Œç¬¬ k ä¸ªæ ·æœ¬çš„ mask ä½ç½®æ˜¯ i + k + 1
            # å¯¹åº”çš„çœŸå® token ä¹Ÿæ˜¯ input_ids[0, i + k + 1]
            
            for k in range(chunk_ids.shape[0]):
                token_idx = i + k + 1
                target_token_id = input_ids[0, token_idx]
                token_logits = logits[k, token_idx, :]
                
                # CrossEntropy = -log_softmax
                log_probs = torch.log_softmax(token_logits, dim=-1)
                total_loss += -log_probs[target_token_id].item()

        # 5. å¹³å‡ Loss
        avg_loss = total_loss / (seq_len - 2)
        nlls.append(avg_loss)

    if not nlls: return 0.0
    # PPL = exp(mean(losses))
    return math.exp(sum(nlls) / len(nlls))

def calculate_visual_score(img_paths, texts, model, processor, device):
    score_sum, count = 0, 0
    for img_path, txt in zip(img_paths, texts):
        try:
            if not os.path.exists(img_path): continue
            image = Image.open(img_path).convert("RGB")
            inputs = processor(text=[txt[:64]], images=image, return_tensors="pt", padding="max_length", truncation=True).to(device)
            with torch.no_grad():
                outputs = model(**inputs)
                img_emb = outputs.image_embeds / outputs.image_embeds.norm(p=2, dim=-1, keepdim=True)
                txt_emb = outputs.text_embeds / outputs.text_embeds.norm(p=2, dim=-1, keepdim=True)
                score = (img_emb @ txt_emb.T).item()
                score_sum += score
                count += 1
        except: pass
    return score_sum / count if count > 0 else 0

def compute_metrics(df, kiwi_model, ppl_bundle, visual_bundle):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    refs = df['ref'].tolist()
    hyps = df['hyp'].tolist()
    srcs = df['src'].tolist()
    img_paths = df['image_path'].tolist()

    kiwi = 0.0
    if kiwi_model:
        try:
            data = [{"src": s, "mt": h} for s, h in zip(srcs, hyps)]
            out = kiwi_model.predict(data, batch_size=32, gpus=1, progress_bar=False)
            kiwi = out.system_score
        except: pass

    bert = 0.0
    if HAS_BERTSCORE:
        try:
            P, R, F1 = run_bert_score(hyps, refs, model_type=XLMR_PATH, num_layers=17, device=device, batch_size=32, verbose=False)
            bert = F1.mean().item()
        except: pass

    ppl = 0.0
    if ppl_bundle:
        try: 
            # ğŸ”¥ ä½¿ç”¨ Pseudo-PPL è®¡ç®—
            ppl = calculate_pseudo_ppl(hyps, ppl_bundle[0], ppl_bundle[1])
        except: pass

    sig_score, clip_base, clip_large = 0.0, 0.0, 0.0
    if 'siglip' in visual_bundle: sig_score = calculate_visual_score(img_paths, hyps, visual_bundle['siglip'][0], visual_bundle['siglip'][1], device)
    if 'clip_base' in visual_bundle: clip_base = calculate_visual_score(img_paths, hyps, visual_bundle['clip_base'][0], visual_bundle['clip_base'][1], device)
    if 'clip_large' in visual_bundle: clip_large = calculate_visual_score(img_paths, hyps, visual_bundle['clip_large'][0], visual_bundle['clip_large'][1], device)

    return ppl, bert, kiwi, sig_score, clip_base, clip_large

def main():
    if not os.path.exists(BASELINE_FILE) or not os.path.exists(FINETUNED_FILE):
        print(f"âŒ Input files not found.")
        return

    kiwi, ppl_bundle, visual_bundle = load_models()
    
    print("\nğŸ“– Reading Data...")
    with open(BASELINE_FILE, 'r') as f: df_base = pd.DataFrame(json.load(f))
    with open(FINETUNED_FILE, 'r') as f: df_tune = pd.DataFrame(json.load(f))
    
    results = []
    all_langs = sorted(df_base['language'].unique().tolist())
    
    def add_row(lang, model_name, metrics):
        results.append({"Language": lang, "Model": model_name, "PPL": metrics[0], "BERTScore": metrics[1], "Kiwi": metrics[2], "SigLIP": metrics[3], "CLIP-B": metrics[4], "CLIP-L": metrics[5]})

    print(f"\nğŸ“Š Processing [GLOBAL AVERAGE] ...")
    b = compute_metrics(df_base, kiwi, ppl_bundle, visual_bundle)
    t = compute_metrics(df_tune, kiwi, ppl_bundle, visual_bundle)
    add_row("AVERAGE", "Baseline", b)
    add_row("", "Ours", t)

    for lang in tqdm(all_langs, desc="Processing"):
        sub_base = df_base[df_base['language'] == lang]
        sub_tune = df_tune[df_tune['language'] == lang]
        if len(sub_base) == 0: continue
        b = compute_metrics(sub_base, kiwi, ppl_bundle, visual_bundle)
        t = compute_metrics(sub_tune, kiwi, ppl_bundle, visual_bundle)
        add_row(lang, "Baseline", b)
        add_row("", "Ours", t)

    df = pd.DataFrame(results)
    pd.set_option('display.max_rows', None)
    pd.set_option('display.float_format', '{:.4f}'.format)
    pd.set_option('display.width', 1200)
    print("\n" + "="*120)
    print("ğŸ† FINAL RESULTS ğŸ†")
    print("="*120)
    print(df.to_string(index=False))
    df.to_csv(OUTPUT_CSV, index=False)

if __name__ == "__main__":
    main()