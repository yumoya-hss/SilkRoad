import json
import torch
import os
from PIL import Image
from tqdm import tqdm
from transformers import AutoModelForImageTextToText, AutoProcessor

# ================= ğŸ”´ é…ç½®åŒºåŸŸ ğŸ”´ =================
# 1. âœ… èåˆåçš„å¾®è°ƒæ¨¡å‹è·¯å¾„ (ç›´æ¥åŠ è½½ï¼Œæ— éœ€ PeftModel)
MODEL_PATH = "/mnt/raid/hss/model/SilkRoad-MMT-8B"

# 2. è¾“å…¥æ•°æ®æ–‡ä»¶
INPUT_JSON = "/home/houshuoshuo/qlora_data/test/multi_100_en.json"

# 3. ç»“æœä¿å­˜æ–‡ä»¶å (å¾®è°ƒç‰ˆ)
OUTPUT_FILE = "pred_100_finetuned_multi.json"

# 4. ç›®æ ‡è¯­è¨€é…ç½®
TARGET_LANGUAGES = {
    'ug': 'Uyghur',
    'uz': 'Uzbek',
    'kk': 'Kazakh',
    'ur': 'Urdu',
    'ky': 'Kyrgyz',
    'tg': 'Tajik'
}
# ===================================================

def generate_response(model, processor, image, prompt_text):
    """
    é€šç”¨ç”Ÿæˆå‡½æ•°
    """
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": prompt_text}
            ]
        }
    ]
    
    # æ„é€ è¾“å…¥
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = processor(
        text=[text],
        images=[image],
        padding=True,
        return_tensors="pt",
    ).to(model.device)

    # æ¨ç†
    with torch.no_grad():
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=False  # è´ªå©ªæœç´¢ï¼Œä¿è¯ç»“æœç¨³å®š
        )

    # è§£ç 
    generated_ids_trimmed = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = processor.batch_decode(
        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )[0]
    
    return output_text

def run_inference():
    print(f"â³ Loading Merged Model from: {MODEL_PATH}")
    
    # 1. ç›´æ¥ä»èåˆæ¨¡å‹è·¯å¾„åŠ è½½ Processor å’Œ Model
    processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)
    
    model = AutoModelForImageTextToText.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.bfloat16,
        attn_implementation="sdpa",
        device_map="auto",
        trust_remote_code=True
    )
    model.eval()
    print("âœ… Merged Model loaded successfully.")

    # è¯»å–è¾“å…¥æ•°æ®
    if not os.path.exists(INPUT_JSON):
        print(f"âŒ Error: Input file not found at {INPUT_JSON}")
        return

    with open(INPUT_JSON, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"ğŸ“„ Loaded {len(data)} items from {INPUT_JSON}")
    
    all_results = []

    # éå†æ¯æ¡æ•°æ®
    for item in tqdm(data, desc="Processing Multi30k (Finetuned)"):
        # 1. è·å–å›¾ç‰‡è·¯å¾„
        raw_path = item.get('saved_path', '')
        image_path = raw_path.replace('//', '/')
        
        # 2. è·å–æºè‹±æ–‡æ–‡æœ¬
        src_en = item.get('src_en', '')
        
        if not os.path.exists(image_path):
            print(f"âš ï¸ Image not found: {image_path}, skipping...")
            continue

        try:
            image = Image.open(image_path).convert("RGB")
            
            # 3. éå† 6 ç§ç›®æ ‡è¯­è¨€è¿›è¡Œç¿»è¯‘
            for lang_code, target_lang_name in TARGET_LANGUAGES.items():
                
                # æ„é€ ç¿»è¯‘æŒ‡ä»¤
                trans_prompt = f"Please translate the description of this image into {target_lang_name}.\nEnglish Source: {src_en}"
                
                # æ‰§è¡Œæ¨ç†
                hyp_translation = generate_response(
                    model, processor, image, trans_prompt
                )
                
                # å­˜å…¥ç»“æœ
                all_results.append({
                    "language": lang_code,
                    "image_id": item.get('image_id'),
                    "image_path": image_path,
                    "src_prompt": trans_prompt, 
                    "src_en": src_en,           
                    "ref": "",                  
                    "hyp": hyp_translation      
                })

        except Exception as e:
            print(f"âŒ Error processing {image_path}: {e}")
            continue

    # ä¿å­˜
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… Finished! Generated {len(all_results)} entries.")
    print(f"   (50 items * 6 langs = 300 expected)")
    print(f"ğŸ“‚ Saved to: {OUTPUT_FILE}")

if __name__ == "__main__":
    run_inference()
