# import json
# import matplotlib.pyplot as plt
# import numpy as np
# import os
# from scipy.stats import gaussian_kde
#
# # ==========================================
# # 1. Load Data (è¯»å–çœŸå® JSON æ–‡ä»¶)
# # ==========================================
# INPUT_FILE = "dataset_optimal_filtered.json"
#
# print(f"ğŸ“– æ­£åœ¨è¯»å–æ–‡ä»¶: {INPUT_FILE} ...")
#
# if not os.path.exists(INPUT_FILE):
#     print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {INPUT_FILE}")
#     print("è¯·ç¡®ä¿ json æ–‡ä»¶ä¸è„šæœ¬åœ¨åŒä¸€ç›®å½•ä¸‹ã€‚")
#     exit(1)
#
# try:
#     with open(INPUT_FILE, 'r', encoding='utf-8') as f:
#         # å°è¯•è¯»å–ï¼Œå…¼å®¹ JSON Array å’Œ JSONL
#         first_char = f.read(1)
#         f.seek(0)
#         if first_char == '[':
#             data = json.load(f)
#         else:
#             data = [json.loads(line) for line in f if line.strip()]
#     print(f"âœ… æˆåŠŸåŠ è½½ {len(data)} æ¡æ•°æ®ã€‚")
# except Exception as e:
#     print(f"âŒ è¯»å– JSON å¤±è´¥: {e}")
#     exit(1)
#
# # ==========================================
# # 2. Extract Lengths (æå–é•¿åº¦)
# # ==========================================
# short_lengths = []
# long_lengths = []
#
# print("âš™ï¸ æ­£åœ¨ç»Ÿè®¡å¥å­é•¿åº¦...")
# for item in data:
#     if 'translations' not in item:
#         continue
#
#     for lang, content in item['translations'].items():
#         # Process Short Captions
#         if 'short_translation' in content and content['short_translation']:
#             text = content['short_translation']
#             # Calculate length by splitting on whitespace (approximate word count)
#             # é’ˆå¯¹ä¸­æ–‡ç­‰æ— ç©ºæ ¼è¯­è¨€ï¼Œlen(split)å¯èƒ½ä¸º1ï¼Œè¿™æ˜¯é¢„æœŸè¡Œä¸ºï¼ˆç»Ÿè®¡ä¸º1å¥ï¼‰
#             # å¦‚æœéœ€è¦æ›´ç²¾ç»†ï¼Œå¯ä»¥æ ¹æ®è¯­è¨€åˆ¤æ–­
#             length = len(text.split())
#             short_lengths.append(length)
#
#         # Process Long Captions
#         if 'long_translation' in content and content['long_translation']:
#             text = content['long_translation']
#             length = len(text.split())
#             long_lengths.append(length)
#
# print(f"ğŸ“Š ç»Ÿè®¡å®Œæˆ: Shortæ ·æœ¬æ•°={len(short_lengths)}, Longæ ·æœ¬æ•°={len(long_lengths)}")
#
# # è½¬ä¸º Numpy æ•°ç»„ï¼Œç¡®ä¿ç»˜å›¾å®‰å…¨
# short_data = np.array(short_lengths)
# long_data = np.array(long_lengths)
#
# # ==========================================
# # 3. Plot Histogram (çº¯ Matplotlib å®‰å…¨ç‰ˆ)
# # ==========================================
# print("ğŸ¨ æ­£åœ¨ç»˜å›¾...")
#
# # è®¾ç½®ç”»æ¿
# fig, ax = plt.subplots(figsize=(8, 5))
#
# # ----------------------------------------
# # ç»˜åˆ¶ Short Caption (è“è‰²)
# # ----------------------------------------
# if len(short_data) > 0:
#     # 1. ç»˜åˆ¶ç›´æ–¹å›¾ (Histogram)
#     ax.hist(short_data, bins=15, density=True, alpha=0.5,
#             color='#3498db', label='Short Caption', edgecolor='white')
#
#     # 2. ç»˜åˆ¶ KDE æ›²çº¿ (æ‰‹åŠ¨è®¡ç®—ï¼Œé¿å¼€ Seaborn é”™è¯¯)
#     try:
#         density_short = gaussian_kde(short_data)
#         # ç”Ÿæˆ X è½´åæ ‡ç‚¹
#         xs_short = np.linspace(0, max(short_data) * 1.2, 200)
#         # ç»˜åˆ¶æ›²çº¿
#         ax.plot(xs_short, density_short(xs_short), color='#3498db', linewidth=2)
#     except Exception as e:
#         print(f"âš ï¸ Short KDE ç»˜åˆ¶å¤±è´¥ (æ•°æ®å¯èƒ½å¤ªå°‘): {e}")
#
# # ----------------------------------------
# # ç»˜åˆ¶ Long Caption (æ©™è‰²)
# # ----------------------------------------
# if len(long_data) > 0:
#     # 1. ç»˜åˆ¶ç›´æ–¹å›¾
#     ax.hist(long_data, bins=25, density=True, alpha=0.5,
#             color='#e67e22', label='Long Caption', edgecolor='white')
#
#     # 2. ç»˜åˆ¶ KDE æ›²çº¿
#     try:
#         density_long = gaussian_kde(long_data)
#         xs_long = np.linspace(0, max(long_data) * 1.2, 200)
#         ax.plot(xs_long, density_long(xs_long), color='#e67e22', linewidth=2)
#     except Exception as e:
#         print(f"âš ï¸ Long KDE ç»˜åˆ¶å¤±è´¥: {e}")
#
# # ----------------------------------------
# # ç¾åŒ–å›¾è¡¨ (ACL é£æ ¼) - å­—ä½“åŠ å¤§ç‰ˆ
# # ----------------------------------------
# # Increased font sizes for better visibility in papers
# ax.set_xlabel("Sentence Length (Number of Tokens)", fontsize=16)   # fontweight='bold'
# ax.set_ylabel("Density", fontsize=16)              # fontweight='bold'
# ax.set_xlim(0, 80)
# ax.set_title("Sentence Length Distribution: Dual-Granularity", fontsize=18, pad=15)
# ax.legend(fontsize=14, loc='upper right')
#
# # Increase tick label size
# ax.tick_params(axis='both', which='major', labelsize=14)
#
# ax.grid(axis='y', linestyle='--', alpha=0.5)
#
# # è°ƒæ•´å¸ƒå±€å¹¶ä¿å­˜
# plt.tight_layout()
#
# # ä¿å­˜
# try:
#     plt.savefig("fig_len_dist.pdf", dpi=300)
#     # plt.savefig("fig_len_dist.png", dpi=300)
#     print("âœ… å›¾ç‰‡å·²ç”Ÿæˆ: fig_len_dist.png / pdf")
# except Exception as e:
#     print(f"âŒ ä¿å­˜å›¾ç‰‡å¤±è´¥: {e}")
#
# # æ˜¾ç¤ºå›¾ç‰‡ (å¦‚æœåœ¨æœåŠ¡å™¨æ— å›¾å½¢ç•Œé¢è¿è¡Œï¼Œè¿™è¡Œå¯èƒ½ä¼šæŠ¥é”™ï¼Œå¯æ³¨é‡Šæ‰)
# try:
#     plt.show()
# except:
#     pass

import json
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
from collections import defaultdict
from scipy.stats import gaussian_kde

# ==========================================
# 1. Load Data (è¯»å–çœŸå® JSON æ–‡ä»¶)
# ==========================================
INPUT_FILE = "dataset_optimal_filtered.json"

print(f"ğŸ“– æ­£åœ¨è¯»å–æ–‡ä»¶: {INPUT_FILE} ...")

if not os.path.exists(INPUT_FILE):
    print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {INPUT_FILE}")
    print("è¯·ç¡®ä¿ json æ–‡ä»¶ä¸è„šæœ¬åœ¨åŒä¸€ç›®å½•ä¸‹ã€‚")
    exit(1)

try:
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        first_char = f.read(1)
        f.seek(0)
        if first_char == '[':
            data = json.load(f)
        else:
            data = [json.loads(line) for line in f if line.strip()]
    print(f"âœ… æˆåŠŸåŠ è½½ {len(data)} æ¡æ•°æ®ã€‚")
except Exception as e:
    print(f"âŒ è¯»å– JSON å¤±è´¥: {e}")
    exit(1)

# ==========================================
# 2. Extract Lengths & Statistics (æå–é•¿åº¦å¹¶ç»Ÿè®¡)
# ==========================================
# å…¨å±€åˆ—è¡¨ (ç”¨äºç”»å›¾)
global_short_lengths = []
global_long_lengths = []

# åˆ†è¯­è¨€ç»Ÿè®¡å­—å…¸: stats[lang]['short'] = [len1, len2...]
lang_stats = defaultdict(lambda: {'short': [], 'long': []})

print("âš™ï¸ æ­£åœ¨ç»Ÿè®¡å¥å­é•¿åº¦...")
for item in data:
    if 'translations' not in item:
        continue

    for lang, content in item['translations'].items():
        # ç»Ÿä¸€è¯­è¨€é”®å (å°å†™)
        lang_key = lang.lower()

        # Process Short Captions
        if 'short_translation' in content and content['short_translation']:
            text = content['short_translation']
            length = len(text.strip().split())  # æŒ‰ç©ºæ ¼åˆ†è¯ç»Ÿè®¡é•¿åº¦

            lang_stats[lang_key]['short'].append(length)
            global_short_lengths.append(length)

        # Process Long Captions
        if 'long_translation' in content and content['long_translation']:
            text = content['long_translation']
            length = len(text.strip().split())

            lang_stats[lang_key]['long'].append(length)
            global_long_lengths.append(length)

print(f"ğŸ“Š ç»Ÿè®¡å®Œæˆ: Shortæ€»æ ·æœ¬æ•°={len(global_short_lengths)}, Longæ€»æ ·æœ¬æ•°={len(global_long_lengths)}")

# ==========================================
# 3. Output Table (è¾“å‡ºè¡¨æ ¼)
# ==========================================
print("\n" + "=" * 60)
print("ğŸ“Š å¹³å‡é•¿åº¦ç»Ÿè®¡ (Average Sentence Length)")
print("=" * 60)

table_rows = []
custom_order = ["uyghur", "kazakh", "kyrgyz", "tajik", "uzbek", "urdu"]
sorted_langs = sorted(lang_stats.keys())  # æˆ–è€…ä½¿ç”¨ custom_order æ’åºé€»è¾‘

# 1. éå†å„è¯­è¨€
for lang in sorted_langs:
    shorts = lang_stats[lang]['short']
    longs = lang_stats[lang]['long']

    avg_s = np.mean(shorts) if shorts else 0.0
    avg_l = np.mean(longs) if longs else 0.0

    table_rows.append({
        "Language": lang.capitalize(),
        "Avg Length (Short)": avg_s,
        "Avg Length (Long)": avg_l,
        "Count (Short)": len(shorts),
        "Count (Long)": len(longs)
    })

# 2. æ·»åŠ å…¨å±€å¹³å‡è¡Œ (Average)
avg_row = {
    "Language": "AVERAGE",
    "Avg Length (Short)": np.mean(global_short_lengths) if global_short_lengths else 0.0,
    "Avg Length (Long)": np.mean(global_long_lengths) if global_long_lengths else 0.0,
    "Count (Short)": len(global_short_lengths),
    "Count (Long)": len(global_long_lengths)
}
table_rows.append(avg_row)

# 3. ç”Ÿæˆ Pandas DataFrame å¹¶æ‰“å°
df = pd.DataFrame(table_rows)

# è®¾ç½®æ˜¾ç¤ºæ ¼å¼ï¼šæµ®ç‚¹æ•°ä¿ç•™2ä½
pd.options.display.float_format = '{:.2f}'.format
# è®¾ç½®åˆ—å¯¹é½å’Œå®½åº¦
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1000)

print(df.to_string(index=False))
print("=" * 60 + "\n")

# å¯é€‰ï¼šä¿å­˜è¡¨æ ¼åˆ° CSV
df.to_csv("length_statistics.csv", index=False)
print("âœ… è¡¨æ ¼æ•°æ®å·²ä¿å­˜è‡³ length_statistics.csv")

# ==========================================
# 4. Plot Histogram (ç»˜å›¾ - ä¿æŒä¸å˜)
# ==========================================
print("ğŸ¨ æ­£åœ¨ç»˜å›¾...")

short_data = np.array(global_short_lengths)
long_data = np.array(global_long_lengths)

# è®¾ç½®ç”»æ¿
fig, ax = plt.subplots(figsize=(8, 5))

# ç»˜åˆ¶ Short Caption (è“è‰²)
if len(short_data) > 0:
    ax.hist(short_data, bins=15, density=True, alpha=0.5,
            color='#3498db', label='Short Caption', edgecolor='white')
    try:
        density_short = gaussian_kde(short_data)
        xs_short = np.linspace(0, max(short_data) * 1.2, 200)
        ax.plot(xs_short, density_short(xs_short), color='#3498db', linewidth=2)
    except Exception as e:
        print(f"âš ï¸ Short KDE ç»˜åˆ¶å¤±è´¥: {e}")

# ç»˜åˆ¶ Long Caption (æ©™è‰²)
if len(long_data) > 0:
    ax.hist(long_data, bins=25, density=True, alpha=0.5,
            color='#e67e22', label='Long Caption', edgecolor='white')
    try:
        density_long = gaussian_kde(long_data)
        xs_long = np.linspace(0, max(long_data) * 1.2, 200)
        ax.plot(xs_long, density_long(xs_long), color='#e67e22', linewidth=2)
    except Exception as e:
        print(f"âš ï¸ Long KDE ç»˜åˆ¶å¤±è´¥: {e}")

# ç¾åŒ–å›¾è¡¨
ax.set_xlabel("Sentence Length (Number of Tokens)", fontsize=16)
ax.set_ylabel("Density", fontsize=16)
ax.set_xlim(0, 80)
ax.set_title("Sentence Length Distribution: Dual-Granularity", fontsize=18, pad=15)
ax.legend(fontsize=14, loc='upper right')
ax.tick_params(axis='both', which='major', labelsize=14)
ax.grid(axis='y', linestyle='--', alpha=0.5)

plt.tight_layout()

try:
    plt.savefig("fig_len_dist.pdf", dpi=300)
    print("âœ… å›¾ç‰‡å·²ç”Ÿæˆ: fig_len_dist.pdf")
except Exception as e:
    print(f"âŒ ä¿å­˜å›¾ç‰‡å¤±è´¥: {e}")

# plt.show()