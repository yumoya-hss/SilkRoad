import json
import sys
import os
import torch
import argparse
import gc
from tqdm import tqdm
from transformers import (
    AutoTokenizer, 
    AutoModelForSeq2SeqLM
)

# ==========================================
# 0. ç”¨æˆ·ç¡¬ç¼–ç é…ç½®åŒºåŸŸ
# ==========================================
# æ¨¡å‹è·¯å¾„ (ä»…ä¿ç•™ NLLB)
NLLB_PATH = "/mnt/raid/hss/model/nllb-200-3.3B"

# æ‰¹å¤„ç†å¤§å°
BATCH_SIZE = 64

# ==========================================
# 1. è¯­è¨€ä»£ç æ˜ å°„è¡¨ (NLLB Codes)
# ==========================================
SUPPORTED_LANGS = {
    # --- ä¸­äºš (Central Asia) ---
    "uzbek": "uzn_Latn",      # ä¹Œå…¹åˆ«å…‹è¯­
    "kazakh": "kaz_Cyrl",     # å“ˆè¨å…‹è¯­
    "kyrgyz": "kir_Cyrl",     # å‰å°”å‰æ–¯è¯­
    "tajik": "tgk_Cyrl",      # å¡”å‰å…‹è¯­

    # --- å—äºš (South Asia) ---
    "urdu": "urd_Arab",       # ä¹Œå°”éƒ½è¯­
    "bengali": "ben_Beng",    # å­ŸåŠ æ‹‰è¯­
    "pashto": "pbt_Arab",     # æ™®ä»€å›¾è¯­
    "hindi": "hin_Deva",      # å°åœ°è¯­
    "nepali": "npi_Deva",     # å°¼æ³Šå°”è¯­
    "marathi": "mar_Deva",    # é©¬æ‹‰åœ°è¯­
    "telugu": "tel_Telu",     # æ³°å¢å›ºè¯­
    "tamil": "tam_Taml",      # æ³°ç±³å°”è¯­

    # --- ä¸œå—äºš (Southeast Asia) ---
    "vietnamese": "vie_Latn", # è¶Šå—è¯­
    "thai": "tha_Thai",       # æ³°è¯­
    "indonesian": "ind_Latn", # å°å°¼è¯­
    "khmer": "khm_Khmr",      # é«˜æ£‰è¯­
    "lao": "lao_Laoo",        # è€æŒè¯­
    "burmese": "mya_Mymr",    # ç¼…ç”¸è¯­
    "malay": "zsm_Latn",      # é©¬æ¥è¯­

    # --- ä¸­ä¸œ/è¥¿äºš (Middle East) ---
    "persian": "pes_Arab",    # æ³¢æ–¯è¯­
    "arabic": "arb_Arab",     # é˜¿æ‹‰ä¼¯è¯­
    "turkish": "tur_Latn",    # åœŸè€³å…¶è¯­
    "hebrew": "heb_Hebr",     # å¸Œä¼¯æ¥è¯­

    # --- éæ´² (Africa) ---
    "swahili": "swh_Latn",    # æ–¯ç“¦å¸Œé‡Œè¯­
    "yoruba": "yor_Latn",     # çº¦é²å·´è¯­
    "zulu": "zul_Latn",       # ç¥–é²è¯­
    "amharic": "amh_Ethi",    # é˜¿å§†å“ˆæ‹‰è¯­
    "hausa": "hau_Latn",      # è±ªè¨è¯­

    # --- ä¸œäºš/å…¶å®ƒ (East Asia / Others) ---
    "uyghur": "uig_Arab",     # ç»´å¾å°”è¯­
    "mongolian": "mon_Cyrl",  # è’™å¤è¯­
    "korean": "kor_Hang",     # éŸ©è¯­
    "japanese": "jpn_Jpan",   # æ—¥è¯­
    "chinese": "zho_Hans",    # ä¸­æ–‡
}

def clean_text(text, lang_code):
    """æ¸…ç† NLLB å¯èƒ½äº§ç”Ÿçš„è¯­è¨€ä»£ç å‰ç¼€"""
    if not text: return ""
    text = text.strip()
    if text.startswith(lang_code):
        text = text[len(lang_code):].strip()
    return text

def load_data(file_path):
    print(f"ğŸ“– è¯»å–æ•°æ®: {file_path} ...")
    data = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            first_char = f.read(1)
            f.seek(0)
            if first_char == '[':
                data = json.load(f)
            else:
                for line in f:
                    if line.strip(): data.append(json.loads(line))
    except Exception as e:
        print(f"âŒ æ•°æ®é”™è¯¯: {e}")
        sys.exit(1)
    print(f"âœ… æˆåŠŸåŠ è½½ {len(data)} æ¡æ•°æ®")
    return data

# ==========================================
# 2. ç¿»è¯‘æ ¸å¿ƒå‡½æ•° (ä»… NLLB é€»è¾‘)
# ==========================================
def translate_dataset(model, tokenizer, data, target_code, device):
    print(f"   >> [NLLB] Translating to code: {target_code} ...")
    
    short_results = []
    long_results = []
    
    # è·å– NLLB çš„ forced_bos_token_id (å¼ºåˆ¶ç›®æ ‡è¯­è¨€å¼€å¤´)
    nllb_bos_id = tokenizer.convert_tokens_to_ids(target_code)

    for i in tqdm(range(0, len(data), BATCH_SIZE), desc=f"   Processing"):
        batch = data[i : i + BATCH_SIZE]
        
        # æå–æ–‡æœ¬
        short_src = [item.get('short_caption_best', item.get('short_caption', "")) for item in batch]
        long_src = [item.get('long_caption_best', item.get('long_caption', "")) for item in batch]
        
        def run_gen(texts, max_new_tokens):
            # è¿‡æ»¤ç©ºæ–‡æœ¬
            valid_indices = [i for i, t in enumerate(texts) if t.strip()]
            if not valid_indices: return [""] * len(texts)
            
            valid_texts = [texts[i] for i in valid_indices]
            # NLLB è¾“å…¥ä¸éœ€è¦ç‰¹æ®Šå‰ç¼€ï¼Œç›´æ¥è¾“å…¥æºæ–‡æœ¬å³å¯ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨å¤„ç†
            inputs = tokenizer(valid_texts, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)
            
            gen_kwargs = {
                "max_new_tokens": max_new_tokens,
                "num_beams": 5,
                "do_sample": False,
                "use_cache": True,
                "forced_bos_token_id": nllb_bos_id  # å…³é”®ï¼šæŒ‡å®šç›®æ ‡è¯­è¨€
            }
            
            with torch.inference_mode():
                out = model.generate(**inputs, **gen_kwargs)
            
            decoded = tokenizer.batch_decode(out, skip_special_tokens=True)
            
            # è¿˜åŸé¡ºåº + æ¸…æ´—
            final_batch_res = [""] * len(texts)
            for idx, res in zip(valid_indices, decoded):
                final_batch_res[idx] = clean_text(res, target_code)
            return final_batch_res
        
        # Short (96 tokens), Long (256 tokens)
        short_results.extend(run_gen(short_src, 96)) 
        long_results.extend(run_gen(long_src, 256)) 
        
    return short_results, long_results

# ==========================================
# ä¸»ç¨‹åº
# ==========================================
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_file", type=str, required=True, help="è¾“å…¥æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_file", type=str, required=True, help="æœ€ç»ˆç»“æœä¿å­˜è·¯å¾„")
    parser.add_argument("--langs", type=str, required=True, help="é€—å·åˆ†éš”çš„è¯­è¨€åˆ—è¡¨, ä¾‹å¦‚: uyghur,uzbek,urdu")
    parser.add_argument("--gpu_id", type=int, default=0)
    args = parser.parse_args()

    # 1. è§£æç›®æ ‡è¯­è¨€
    target_lang_names = [l.strip().lower() for l in args.langs.split(',')]
    valid_langs = []
    
    print("\nğŸ” æ£€æŸ¥è¯­è¨€åˆ—è¡¨...")
    for lang in target_lang_names:
        if lang not in SUPPORTED_LANGS:
            print(f"âŒ è­¦å‘Š: ä¸æ”¯æŒçš„è¯­è¨€ '{lang}'ï¼Œå°†è·³è¿‡ã€‚")
        else:
            print(f"âœ… å¾…å¤„ç†: {lang:<12} (Code: {SUPPORTED_LANGS[lang]})")
            valid_langs.append(lang)
            
    if not valid_langs:
        print("æ²¡æœ‰æœ‰æ•ˆçš„è¯­è¨€ï¼Œé€€å‡ºã€‚")
        return

    device = f"cuda:{args.gpu_id}" if torch.cuda.is_available() else "cpu"
    data = load_data(args.input_file)

    # ç»“æœç¼“å­˜: results_cache[lang] = {'short': [], 'long': []}
    results_cache = {lang: {} for lang in valid_langs}

    # =========================================================
    # Phase 1: åŠ è½½ NLLB-200 (ä¸€æ¬¡åŠ è½½ï¼Œå¾ªç¯å¤šè¯­)
    # =========================================================
    print(f"\n[{device}] Loading NLLB Model from {NLLB_PATH} ...")
    try:
        n_model = AutoModelForSeq2SeqLM.from_pretrained(
            NLLB_PATH, 
            torch_dtype=torch.float16,
            attn_implementation="flash_attention_2" if torch.cuda.get_device_capability()[0] >= 8 else "eager"
        ).to(device).eval()
    except:
        print("âš ï¸ Flash Attention 2 åŠ è½½å¤±è´¥æˆ–ä¸æ”¯æŒï¼Œå›é€€åˆ°é»˜è®¤æ¨¡å¼ã€‚")
        n_model = AutoModelForSeq2SeqLM.from_pretrained(NLLB_PATH, torch_dtype=torch.float16).to(device).eval()
    
    n_tok = AutoTokenizer.from_pretrained(NLLB_PATH)

    # å¾ªç¯ç¿»è¯‘æ‰€æœ‰è¯­è¨€
    for lang in valid_langs:
        target_code = SUPPORTED_LANGS[lang]
        # NLLB æ”¯æŒæ‰€æœ‰åˆ—è¡¨ä¸­çš„è¯­è¨€
        n_short, n_long = translate_dataset(n_model, n_tok, data, target_code, device)
        results_cache[lang]['short'] = n_short
        results_cache[lang]['long'] = n_long

    # å¸è½½æ¨¡å‹ (å¯é€‰ï¼Œå¦‚æœåé¢æ²¡æœ‰å…¶ä»–é‡å‹æ“ä½œ)
    del n_model, n_tok
    torch.cuda.empty_cache()
    gc.collect()

    # =========================================================
    # Phase 2: åˆå¹¶ç»“æœå¹¶ä¿å­˜
    # =========================================================
    print(f"\nğŸ”„ æ­£åœ¨åˆå¹¶å¤šè¯­è¨€æ•°æ®...")
    for idx, item in enumerate(data):
        if 'translations' not in item:
            item['translations'] = {}
        
        # æ³¨å…¥æ‰€æœ‰è¯­è¨€çš„ç¿»è¯‘ç»“æœ (ä»… NLLB)
        for lang in valid_langs:
            item['translations'][lang] = {
                "short_nllb": results_cache[lang]['short'][idx],
                "long_nllb":  results_cache[lang]['long'][idx]
            }

    print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆç»“æœè‡³: {args.output_file}")
    with open(args.output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print("ğŸ‰ NLLB ç¿»è¯‘ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼")

if __name__ == "__main__":
    main()