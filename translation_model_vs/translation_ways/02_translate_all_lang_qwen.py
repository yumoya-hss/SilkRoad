import json
import sys
import os
import torch
import argparse
import gc
import re
from tqdm import tqdm
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM
)

# ==========================================
# 0. ç”¨æˆ·é…ç½®
# ==========================================
QWEN_PATH = "/mnt/raid/zsb/llm_models/Qwen3-32B"

# âš¡ æ‰¹å¤„ç†å¤§å°
# å»ºè®®ï¼š24Gæ˜¾å­˜(4bit)è®¾ä¸º16ï¼›48G+æ˜¾å­˜(FP16)è®¾ä¸º32
# å¦‚æœæ˜¾å­˜ä¸å¤Ÿï¼Œå¯ä»¥æ”¹å°åˆ° 8 æˆ– 4
BATCH_SIZE = 16 

# ==========================================
# 1. è¯­è¨€é…ç½®ä¸è„šæœ¬çº¦æŸ
# ==========================================
VALID_LANG_KEYS = {
    "uzbek", "kazakh", "kyrgyz", "tajik",
    "urdu", "bengali", "pashto", "hindi", "nepali", "marathi", "telugu", "tamil",
    "vietnamese", "thai", "indonesian", "khmer", "lao", "burmese", "malay",
    "persian", "arabic", "turkish", "hebrew",
    "swahili", "yoruba", "zulu", "amharic", "hausa",
    "uyghur", "mongolian", "korean", "japanese", "chinese"
}

QWEN_LANG_SCRIPT_MAP = {
    "uyghur": "Uyghur (in standard Arabic script/UEY)",
    "kazakh": "Kazakh (in Cyrillic script)",
    "uzbek": "Uzbek (in Latin script)",
    "kyrgyz": "Kyrgyz (in Cyrillic script)",
    "tajik": "Tajik (in Cyrillic script)",
    "urdu": "Urdu (in Arabic script)",
    "chinese": "Chinese (Simplified)",
    "mongolian": "Mongolian (in Cyrillic script)",
}

# ==========================================
# 2. å·¥å…·å‡½æ•°
# ==========================================
def load_data(file_path):
    print(f"ğŸ“– è¯»å–æ•°æ®: {file_path} ...")
    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        sys.exit(1)
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read().strip()
            if not content: return []
            if content.startswith('['):
                return json.loads(content)
            else:
                return [json.loads(line) for line in content.split('\n') if line.strip()]
    except Exception as e:
        print(f"âŒ æ•°æ®é”™è¯¯: {e}"); sys.exit(1)

def save_data(data, output_file):
    """ä¿å­˜æ•°æ®åˆ°ç£ç›˜"""
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")

def clean_translation(text):
    """
    æ¸…æ´—å‡½æ•°ï¼šå»é™¤ <think> æ ‡ç­¾ã€Markdown å’Œå¸¸è§å‰ç¼€
    """
    if not text: return ""
    # 1. å»é™¤ <think>...å†…å®¹...</think>
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    # 2. å»é™¤ Markdown
    text = text.replace('```', '').strip()
    # 3. å»é™¤å¸¸è§å‰ç¼€
    prefixes = ["Translation:", "Translated:", "Output:", "Answer:", "ç¿»è¯‘ï¼š", "Result:"]
    for p in prefixes:
        if text.lower().startswith(p.lower()):
            text = text[len(p):].strip()
    # 4. å»é™¤é¦–å°¾å¼•å·
    if text.startswith('"') and text.endswith('"') and len(text) > 2:
        text = text[1:-1]
    return text.strip()

# ==========================================
# 3. ç¿»è¯‘ä¸»é€»è¾‘
# ==========================================
def run_translation(model, tokenizer, data, lang_key, device, output_file):
    target_lang_desc = QWEN_LANG_SCRIPT_MAP.get(lang_key, lang_key.capitalize())
    
    print(f"\nğŸš€ å¼€å§‹ç¿»è¯‘: {lang_key} -> {target_lang_desc}")
    print("ğŸ’¾ æ¨¡å¼: å®æ—¶ä¿å­˜ (æ¯æ‰¹æ¬¡å®Œæˆåç«‹å³å†™å…¥JSON)")
    
    # System Prompt: ä¸“å®¶è§’è‰² + ä¸¥æ ¼çº¦æŸ
    sys_prompt = (
        f"You are a professional linguist and native speaker of {target_lang_desc}.\n"
        "### Task\n"
        f"Translate the English image caption into natural, grammatical {target_lang_desc}.\n\n"
        "### Strict Rules\n"
        "1. **Script Compliance**: Use the OFFICIAL script ONLY (e.g., Arabic for Uyghur). Do NOT use transliteration.\n"
        "2. **Accuracy**: Preserve the exact meaning without hallucination.\n"
        "3. **No Thinking**: Do NOT output <think> tags.\n"
        "4. **Output**: Output ONLY the translation.\n"
    )

    # 1. ç­›é€‰æœªç¿»è¯‘çš„æ•°æ® (æ–­ç‚¹ç»­ä¼ )
    todo_indices = []
    for idx, item in enumerate(data):
        if 'translations' not in item: item['translations'] = {}
        if lang_key not in item['translations']: item['translations'][lang_key] = {}
        
        existing_s = item['translations'][lang_key].get("short_qwen", "")
        existing_l = item['translations'][lang_key].get("long_qwen", "")
        
        if not (existing_s and existing_l):
            todo_indices.append(idx)

    if not todo_indices:
        print(f"âœ… {lang_key} å…¨éƒ¨å·²å®Œæˆ ({len(data)} æ¡)ï¼Œè·³è¿‡ã€‚")
        return

    # 2. å®šä¹‰æ‰¹é‡ç”Ÿæˆå‡½æ•°
    def generate_batch(texts, max_len):
        valid_map = {i: t for i, t in enumerate(texts) if t.strip()}
        if not valid_map: return [""] * len(texts)
        
        prompts = []
        for txt in valid_map.values():
            messages = [{"role": "system", "content": sys_prompt}, {"role": "user", "content": txt}]
            prompts.append(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))

        tokenizer.padding_side = "left"
        inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True, max_length=2048).to(device)

        with torch.inference_mode():
            gen_ids = model.generate(
                **inputs,
                max_new_tokens=max_len + 32,
                temperature=0.0,             # Greedy Decoding (æœ€å¿«)
                do_sample=False,             # å…³é—­é‡‡æ ·
                top_p=1.0,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        gen_ids = [out[len(inp):] for inp, out in zip(inputs.input_ids, gen_ids)]
        decoded = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)
        
        results = [""] * len(texts)
        for i, (v_idx, raw_text) in enumerate(zip(valid_map.keys(), decoded)):
            results[v_idx] = clean_translation(raw_text)
        return results

    # 3. å¾ªç¯å¤„ç†
    steps = 0
    try:
        pbar = tqdm(total=len(todo_indices), desc=f"Processing {lang_key}", mininterval=1.0)
        
        for i in range(0, len(todo_indices), BATCH_SIZE):
            batch_idxs = todo_indices[i : i + BATCH_SIZE]
            
            # æå–åŸæ–‡
            short_src = [data[idx].get('short_caption_best', "") for idx in batch_idxs]
            long_src = [data[idx].get('long_caption_best', "") for idx in batch_idxs]

            # æ‰§è¡Œç¿»è¯‘
            short_out = generate_batch(short_src, 128)
            long_out = generate_batch(long_src, 256)

            # å›å†™æ•°æ®
            for j, data_idx in enumerate(batch_idxs):
                data[data_idx]['translations'][lang_key]['short_qwen'] = short_out[j]
                data[data_idx]['translations'][lang_key]['long_qwen'] = long_out[j]
                
                # [å¯é€‰] æ‰“å°ç¬¬ä¸€æ¡åšç›‘æ§
                if j == 0:
                    tqdm.write(f"ğŸ“ Src: {short_src[j][:30]}...  =>  ğŸŸ¢ Tgt: {short_out[j][:30]}...")

            pbar.update(len(batch_idxs))
            steps += 1

            # ğŸ”¥ [æ ¸å¿ƒä¿®æ”¹] æ¯ä¸€æ‰¹æ¬¡å¤„ç†å®Œï¼Œç«‹å³ä¿å­˜ï¼
            # è¿™æ ·ä½ æ‰“å¼€ JSON æ–‡ä»¶ï¼Œæ°¸è¿œèƒ½çœ‹åˆ°æœ€æ–°çš„ç¿»è¯‘ç»“æœ
            save_data(data, output_file)
                
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·åœæ­¢ï¼æ­£åœ¨æœ€åä¸€æ¬¡ä¿å­˜...")
        save_data(data, output_file)
        sys.exit(0)

    # è·‘å®Œä¸€ç§è¯­è¨€ï¼Œå†æ¬¡ç¡®ä¿ä¿å­˜
    save_data(data, output_file)

# ==========================================
# ä¸»ç¨‹åº
# ==========================================
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_file", type=str, required=True, help="åŸå§‹è¾“å…¥æ–‡ä»¶")
    parser.add_argument("--output_file", type=str, required=True, help="è¾“å‡ºæ–‡ä»¶ (æ”¯æŒå¢é‡)")
    parser.add_argument("--langs", type=str, required=True, help="ç›®æ ‡è¯­è¨€åˆ—è¡¨")
    parser.add_argument("--gpu_id", type=int, default=0)
    args = parser.parse_args()

    langs = [l.strip().lower() for l in args.langs.split(',')]
    valid_langs = [l for l in langs if l in VALID_LANG_KEYS]
    if not valid_langs: 
        print("âŒ æ— æœ‰æ•ˆè¯­è¨€ã€‚")
        return

    # å¢é‡åŠ è½½é€»è¾‘
    if os.path.exists(args.output_file):
        print(f"ğŸ”„ æ£€æµ‹åˆ°è¿›åº¦æ–‡ä»¶: {args.output_file}ï¼ŒåŠ è½½ä»¥ç»§ç»­...")
        data = load_data(args.output_file)
    else:
        print(f"ğŸ†• é¦–æ¬¡è¿è¡Œï¼ŒåŠ è½½åŸå§‹æ–‡ä»¶: {args.input_file}")
        data = load_data(args.input_file)

    device = f"cuda:{args.gpu_id}"
    
    print(f"[{device}] Loading Qwen3-32B...")
    try:
        # æ ‡å‡†åŠ è½½
        model = AutoModelForCausalLM.from_pretrained(
            QWEN_PATH, 
            torch_dtype=torch.float16, 
            device_map=device, 
            trust_remote_code=True
        ).eval()
        tokenizer = AutoTokenizer.from_pretrained(QWEN_PATH, trust_remote_code=True)
        if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # é€ä¸ªè¯­è¨€å¤„ç†
    for lang in valid_langs:
        run_translation(model, tokenizer, data, lang, device, args.output_file)

    print("\nğŸ‰ å…¨éƒ¨ä»»åŠ¡å®Œæˆï¼")

if __name__ == "__main__":
    main()