import json
import sys
import os
import torch
import argparse
import gc
from tqdm import tqdm
from transformers import (
    AutoTokenizer, 
    AutoModelForSeq2SeqLM, 
    SeamlessM4Tv2ForTextToText
)

# ==========================================
# 0. ç”¨æˆ·ç¡¬ç¼–ç é…ç½®åŒºåŸŸ (ç›´æ¥ä¿®æ”¹è¿™é‡Œ)
# ==========================================
# æ¨¡å‹è·¯å¾„
SEAMLESS_PATH = "models/seamless-m4t-v2-large"
NLLB_PATH = "models/nllb-200-3.3B"

# æ‰¹å¤„ç†å¤§å° (æ˜¾å­˜å¤Ÿå¤§å¯è®¾ä¸º 64 æˆ– 128)
BATCH_SIZE = 64

# ==========================================
# 1. è¯­è¨€ä»£ç æ˜ å°„è¡¨ (å®Œæ•´ä¿ç•™)
# ==========================================
SUPPORTED_LANGS = {
    # --- ä¸­äºš (Central Asia) ---
    "uzbek": "uzn_Latn",      # ä¹Œå…¹åˆ«å…‹è¯­
    "kazakh": "kaz_Cyrl",     # å“ˆè¨å…‹è¯­
    "kyrgyz": "kir_Cyrl",     # å‰å°”å‰æ–¯è¯­
    "tajik": "tgk_Cyrl",      # å¡”å‰å…‹è¯­

    # --- å—äºš (South Asia) ---
    "urdu": "urd_Arab",       # ä¹Œå°”éƒ½è¯­
    "bengali": "ben_Beng",    # å­ŸåŠ æ‹‰è¯­
    "pashto": "pbt_Arab",     # æ™®ä»€å›¾è¯­
    "hindi": "hin_Deva",      # å°åœ°è¯­
    "nepali": "npi_Deva",     # å°¼æ³Šå°”è¯­
    "marathi": "mar_Deva",    # é©¬æ‹‰åœ°è¯­
    "telugu": "tel_Telu",     # æ³°å¢å›ºè¯­
    "tamil": "tam_Taml",      # æ³°ç±³å°”è¯­

    # --- ä¸œå—äºš (Southeast Asia) ---
    "vietnamese": "vie_Latn", # è¶Šå—è¯­
    "thai": "tha_Thai",       # æ³°è¯­
    "indonesian": "ind_Latn", # å°å°¼è¯­
    "khmer": "khm_Khmr",      # é«˜æ£‰è¯­
    "lao": "lao_Laoo",        # è€æŒè¯­
    "burmese": "mya_Mymr",    # ç¼…ç”¸è¯­
    "malay": "zsm_Latn",      # é©¬æ¥è¯­

    # --- ä¸­ä¸œ/è¥¿äºš (Middle East) ---
    "persian": "pes_Arab",    # æ³¢æ–¯è¯­
    "arabic": "arb_Arab",     # é˜¿æ‹‰ä¼¯è¯­
    "turkish": "tur_Latn",    # åœŸè€³å…¶è¯­
    "hebrew": "heb_Hebr",     # å¸Œä¼¯æ¥è¯­

    # --- éæ´² (Africa) ---
    "swahili": "swh_Latn",    # æ–¯ç“¦å¸Œé‡Œè¯­
    "yoruba": "yor_Latn",     # çº¦é²å·´è¯­
    "zulu": "zul_Latn",       # ç¥–é²è¯­
    "amharic": "amh_Ethi",    # é˜¿å§†å“ˆæ‹‰è¯­
    "hausa": "hau_Latn",      # è±ªè¨è¯­

    # --- ä¸œäºš/å…¶å®ƒ (East Asia / Others) ---
    "uyghur": "uig_Arab",     # ç»´å¾å°”è¯­ (ä»… NLLB æ”¯æŒ)
    "mongolian": "mon_Cyrl",  # è’™å¤è¯­
    "korean": "kor_Hang",     # éŸ©è¯­
    "japanese": "jpn_Jpan",   # æ—¥è¯­
    "chinese": "zho_Hans",    # ä¸­æ–‡
}

# Seamless ä»£ç æ˜ å°„ä¿®æ­£ (NLLB Code -> Seamless ISO 639-3)
NLLB_TO_SEAMLESS_MAP = {
    # è§„åˆ™æ˜ å°„ (å‰ä¸‰ä½)
    "uzn_Latn": "uzn", "kaz_Cyrl": "kaz", "kir_Cyrl": "kir", "tgk_Cyrl": "tgk",
    "urd_Arab": "urd", "ben_Beng": "ben", "pbt_Arab": "pbt", "hin_Deva": "hin",
    "npi_Deva": "npi", "mar_Deva": "mar", "tel_Telu": "tel", "tam_Taml": "tam",
    "vie_Latn": "vie", "tha_Thai": "tha", "ind_Latn": "ind", "khm_Khmr": "khm",
    "lao_Laoo": "lao", "mya_Mymr": "mya", 
    "pes_Arab": "pes", "arb_Arab": "arb", "tur_Latn": "tur", "heb_Hebr": "heb",
    "swh_Latn": "swh", "yor_Latn": "yor", "zul_Latn": "zul", "amh_Ethi": "amh",
    "mon_Cyrl": "mon", "kor_Hang": "kor", "jpn_Jpan": "jpn",
    # ç‰¹æ®Šæ˜ å°„
    "zho_Hans": "cmn",  # ä¸­æ–‡
    "zsm_Latn": "zlm",  # é©¬æ¥è¯­
    "hau_Latn": "hau",  # è±ªè¨è¯­
    "uig_Arab": "uig",  # ç»´å¾å°”è¯­ (æ³¨æ„: Seamless ä¸æ”¯æŒï¼Œä½†æ˜ å°„è¡¨é‡Œç•™ç€æ–¹ä¾¿é€»è¾‘åˆ¤æ–­)
}

# SeamlessM4T v2 ç™½åå• (ä¸¥æ ¼æ ¡éªŒ)
SEAMLESS_SUPPORTED_TGTS = {
    "afr","amh","arb","ary","arz","asm","azj","bel","ben","bos","bul","cat","ceb","ces",
    "ckb","cmn","cmn_Hant","cym","dan","deu","ell","eng","est","eus","fin","fra","fuv",
    "gaz","gle","glg","guj","heb","hin","hrv","hun","hye","ibo","ind","isl","ita","jav",
    "jpn","kan","kat","kaz","khk","khm","kir","kor","lao","lit","lug","luo","lvs","mai",
    "mal","mar","mkd","mlt","mni","mya","nld","nno","nob","npi","nya","ory","pan","pbt",
    "pes","pol","por","ron","rus","sat","slk","slv","sna","snd","som","spa","srp","swe",
    "swh","tam","tel","tgk","tgl","tha","tur","ukr","urd","uzn","vie","yor","yue","zlm","zul",
    "hau" # è¡¥å……è±ªè¨è¯­
}

def clean_text(text, lang_code):
    if not text: return ""
    text = text.strip()
    if text.startswith(lang_code):
        text = text[len(lang_code):].strip()
    return text

def load_data(file_path):
    print(f"ğŸ“– è¯»å–æ•°æ®: {file_path} ...")
    data = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            first_char = f.read(1)
            f.seek(0)
            if first_char == '[':
                data = json.load(f)
            else:
                for line in f:
                    if line.strip(): data.append(json.loads(line))
    except Exception as e:
        print(f"âŒ æ•°æ®é”™è¯¯: {e}")
        sys.exit(1)
    print(f"âœ… æˆåŠŸåŠ è½½ {len(data)} æ¡æ•°æ®")
    return data

# ==========================================
# 2. ç¿»è¯‘æ ¸å¿ƒå‡½æ•°
# ==========================================
def translate_dataset(model, tokenizer, data, model_type, target_code, device):
    print(f"   >> [{model_type.upper()}] Translating to code: {target_code} ...")
    
    short_results = []
    long_results = []
    
    # å‡†å¤‡å‚æ•°
    seamless_lang = NLLB_TO_SEAMLESS_MAP.get(target_code, target_code[:3])
    nllb_bos_id = None
    if model_type == 'nllb':
        nllb_bos_id = tokenizer.convert_tokens_to_ids(target_code)

    for i in tqdm(range(0, len(data), BATCH_SIZE), desc=f"   Processing"):
        batch = data[i : i + BATCH_SIZE]
        
        # æå–æ–‡æœ¬
        short_src = [item.get('short_caption_best', item.get('short_caption', "")) for item in batch]
        long_src = [item.get('long_caption_best', item.get('long_caption', "")) for item in batch]
        
        def run_gen(texts, max_new_tokens):
            # è¿‡æ»¤ç©ºæ–‡æœ¬
            valid_indices = [i for i, t in enumerate(texts) if t.strip()]
            if not valid_indices: return [""] * len(texts)
            
            valid_texts = [texts[i] for i in valid_indices]
            inputs = tokenizer(valid_texts, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)
            
            gen_kwargs = {
                "max_new_tokens": max_new_tokens,
                "num_beams": 5,
                "do_sample": False,
                "use_cache": True
            }
            
            if model_type == 'seamless':
                gen_kwargs["tgt_lang"] = seamless_lang
            else:
                gen_kwargs["forced_bos_token_id"] = nllb_bos_id
            
            with torch.inference_mode():
                out = model.generate(**inputs, **gen_kwargs)
            
            decoded = tokenizer.batch_decode(out, skip_special_tokens=True)
            
            # è¿˜åŸé¡ºåº + æ¸…æ´—
            final_batch_res = [""] * len(texts)
            for idx, res in zip(valid_indices, decoded):
                if model_type == 'nllb':
                    final_batch_res[idx] = clean_text(res, target_code)
                else:
                    final_batch_res[idx] = res.strip()
            return final_batch_res
        
        # Short (96 tokens), Long (256 tokens) - æœ€ä¼˜é•¿åº¦è®¾ç½®
        short_results.extend(run_gen(short_src, 96)) 
        long_results.extend(run_gen(long_src, 256)) 
        
    return short_results, long_results

# ==========================================
# ä¸»ç¨‹åº
# ==========================================
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_file", type=str, required=True, help="Stage 2/3 çš„è¾“å‡ºæ–‡ä»¶")
    parser.add_argument("--output_file", type=str, required=True, help="æœ€ç»ˆç»“æœä¿å­˜è·¯å¾„")
    parser.add_argument("--langs", type=str, required=True, help="é€—å·åˆ†éš”çš„è¯­è¨€åˆ—è¡¨, ä¾‹å¦‚: uyghur,uzbek,urdu")
    parser.add_argument("--gpu_id", type=int, default=0)
    args = parser.parse_args()

    # 1. è§£æç›®æ ‡è¯­è¨€
    target_lang_names = [l.strip().lower() for l in args.langs.split(',')]
    valid_langs = []
    
    print("\nğŸ” æ£€æŸ¥è¯­è¨€åˆ—è¡¨...")
    for lang in target_lang_names:
        if lang not in SUPPORTED_LANGS:
            print(f"âŒ è­¦å‘Š: ä¸æ”¯æŒçš„è¯­è¨€ '{lang}'ï¼Œå°†è·³è¿‡ã€‚")
        else:
            print(f"âœ… å¾…å¤„ç†: {lang:<12} (Code: {SUPPORTED_LANGS[lang]})")
            valid_langs.append(lang)
            
    if not valid_langs:
        print("æ²¡æœ‰æœ‰æ•ˆçš„è¯­è¨€ï¼Œé€€å‡ºã€‚")
        return

    device = f"cuda:{args.gpu_id}" if torch.cuda.is_available() else "cpu"
    data = load_data(args.input_file)

    # ç»“æœç¼“å­˜: results_cache[lang][model] = {'short': [], 'long': []}
    results_cache = {lang: {'seamless': {}, 'nllb': {}} for lang in valid_langs}

    # =========================================================
    # Phase 1: SeamlessM4T v2 (ä¸€æ¬¡åŠ è½½ï¼Œå¾ªç¯å¤šè¯­)
    # =========================================================
    print(f"\n[{device}] Phase 1: Loading SeamlessM4T...")
    try:
        s_model = SeamlessM4Tv2ForTextToText.from_pretrained(
            SEAMLESS_PATH, 
            torch_dtype=torch.float16, 
            attn_implementation="flash_attention_2" if torch.cuda.get_device_capability()[0] >= 8 else "eager"
        ).to(device).eval()
    except:
        s_model = SeamlessM4Tv2ForTextToText.from_pretrained(SEAMLESS_PATH, torch_dtype=torch.float16).to(device).eval()
    s_tok = AutoTokenizer.from_pretrained(SEAMLESS_PATH)

    # å¾ªç¯ç¿»è¯‘æ‰€æœ‰è¯­è¨€
    for lang in valid_langs:
        target_code = SUPPORTED_LANGS[lang]
        seamless_code = NLLB_TO_SEAMLESS_MAP.get(target_code, target_code[:3])
        
        # æ£€æŸ¥æ”¯æŒæ€§ (ä¾‹å¦‚ç»´å¾å°”è¯­ä¼šè¢«è‡ªåŠ¨è·³è¿‡)
        if seamless_code in SEAMLESS_SUPPORTED_TGTS:
            s_short, s_long = translate_dataset(s_model, s_tok, data, 'seamless', target_code, device)
            results_cache[lang]['seamless']['short'] = s_short
            results_cache[lang]['seamless']['long'] = s_long
        else:
            print(f"âš ï¸ Seamless ä¸æ”¯æŒ {lang} (code: {seamless_code})ï¼Œè·³è¿‡ï¼Œå¡«ç©ºå€¼ã€‚")
            results_cache[lang]['seamless']['short'] = [""] * len(data)
            results_cache[lang]['seamless']['long'] = [""] * len(data)

    # å¸è½½ Seamless
    del s_model, s_tok
    torch.cuda.empty_cache()
    gc.collect()
    print("âœ… Seamless é˜¶æ®µå®Œæˆï¼Œæ˜¾å­˜å·²é‡Šæ”¾ã€‚")

    # =========================================================
    # Phase 2: NLLB-200 (ä¸€æ¬¡åŠ è½½ï¼Œå¾ªç¯å¤šè¯­)
    # =========================================================
    print(f"\n[{device}] Phase 2: Loading NLLB...")
    try:
        n_model = AutoModelForSeq2SeqLM.from_pretrained(
            NLLB_PATH, 
            torch_dtype=torch.float16,
            attn_implementation="flash_attention_2" if torch.cuda.get_device_capability()[0] >= 8 else "eager"
        ).to(device).eval()
    except:
        n_model = AutoModelForSeq2SeqLM.from_pretrained(NLLB_PATH, torch_dtype=torch.float16).to(device).eval()
    n_tok = AutoTokenizer.from_pretrained(NLLB_PATH)

    # å¾ªç¯ç¿»è¯‘æ‰€æœ‰è¯­è¨€
    for lang in valid_langs:
        target_code = SUPPORTED_LANGS[lang]
        # NLLB æ”¯æŒæ‰€æœ‰åˆ—è¡¨ä¸­çš„è¯­è¨€
        n_short, n_long = translate_dataset(n_model, n_tok, data, 'nllb', target_code, device)
        results_cache[lang]['nllb']['short'] = n_short
        results_cache[lang]['nllb']['long'] = n_long

    # å¸è½½ NLLB
    del n_model, n_tok
    torch.cuda.empty_cache()

    # =========================================================
    # Phase 3: åˆå¹¶ç»“æœå¹¶ä¿å­˜
    # =========================================================
    print(f"\nğŸ”„ æ­£åœ¨åˆå¹¶å¤šè¯­è¨€æ•°æ®...")
    for idx, item in enumerate(data):
        if 'translations' not in item:
            item['translations'] = {}
        
        # æ³¨å…¥æ‰€æœ‰è¯­è¨€çš„ç¿»è¯‘ç»“æœ
        for lang in valid_langs:
            item['translations'][lang] = {
                "short_seamless": results_cache[lang]['seamless']['short'][idx],
                "long_seamless":  results_cache[lang]['seamless']['long'][idx],
                "short_nllb":     results_cache[lang]['nllb']['short'][idx],
                "long_nllb":      results_cache[lang]['nllb']['long'][idx]
            }

    print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆç»“æœè‡³: {args.output_file}")
    with open(args.output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print("ğŸ‰ å¤šè¯­è¨€ç¿»è¯‘å…¨éƒ¨å®Œæˆï¼")

if __name__ == "__main__":
    main()