import json
import sys
import argparse
from collections import defaultdict

# ==========================================
# ğŸ”¥ [ä¸¥é€‰é˜ˆå€¼é…ç½®] ğŸ”¥
# åªæœ‰è¶…è¿‡è¿™äº›åˆ†æ•°çš„ç¿»è¯‘æ‰æœ‰èµ„æ ¼è¿›å…¥"å†³èµ›"
# ==========================================
# ==========================================
# ğŸ”¥ [é»„é‡‘é˜ˆå€¼é…ç½®] ğŸ”¥
# è¿™äº›å€¼æ˜¯æ ¹æ®å­¦æœ¯ç»éªŒè®¾å®šçš„"é«˜è´¨é‡"åŸºå‡†çº¿
# ==========================================

# 1. è¯­ä¹‰ä¸€è‡´æ€§ (BERTScore): æœ€é‡è¦çš„æŒ‡æ ‡
# å¦‚æœå›è¯‘éƒ½å¯¹ä¸ä¸Šï¼Œè¯´æ˜ç¿»è¯‘å®Œå…¨é”™äº†ã€‚
THRESHOLD_BERT = 0.88

# 2. ç¿»è¯‘è´¨é‡ (COMET-Kiwi): 
# ä¿è¯è¯‘æ–‡æµç•…ã€è¯­æ³•æ­£ç¡®ã€‚
THRESHOLD_COMET = 0.72

# 3. è§†è§‰ä¸€è‡´æ€§ (CLIP Score): 
# é˜²æ­¢ä¸¥é‡å¹»è§‰ (Hallucination)ã€‚
THRESHOLD_CLIP = 0.22

def load_data(file_path):
    print(f"ğŸ“– è¯»å–æ•°æ®: {file_path} ...")
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            if f.read(1) == '[':
                f.seek(0); return json.load(f)
            f.seek(0); return [json.loads(line) for line in f if line.strip()]
    except Exception as e:
        print(f"âŒ è¯»å–å¤±è´¥: {e}"); sys.exit(1)

def select_best_candidate(trans_obj, type_prefix):
    """
    è¾“å…¥: trans_obj (æŸä¸ªè¯­è¨€çš„æ‰€æœ‰ç¿»è¯‘å­—æ®µ), type_prefix ('short' or 'long')
    è¾“å‡º: (best_text, best_model_name, best_scores) or (None, None, None)
    """
    # 1. å®šä¹‰å‚èµ›é€‰æ‰‹
    candidates = []
    
    # é€‰æ‰‹ A: NLLB
    if f"{type_prefix}_nllb" in trans_obj:
        candidates.append({
            "model": "nllb",
            "text": trans_obj.get(f"{type_prefix}_nllb"),
            "bert": trans_obj.get(f"score_bert_{type_prefix}_nllb", -1),
            "comet": trans_obj.get(f"score_comet_{type_prefix}_nllb", -1),
            "visual": trans_obj.get(f"score_visual_{type_prefix}_nllb", -1)
        })
        
    # é€‰æ‰‹ B: Seamless (å¦‚æœå­˜åœ¨)
    if f"{type_prefix}_seamless" in trans_obj:
        candidates.append({
            "model": "seamless",
            "text": trans_obj.get(f"{type_prefix}_seamless"),
            "bert": trans_obj.get(f"score_bert_{type_prefix}_seamless", -1),
            "comet": trans_obj.get(f"score_comet_{type_prefix}_seamless", -1),
            "visual": trans_obj.get(f"score_visual_{type_prefix}_seamless", -1)
        })

    # 2. èµ„æ ¼èµ› (è¿‡æ»¤æ‰ä¸åŠæ ¼çš„)
    qualified = []
    for cand in candidates:
        if not cand['text']: continue
        # å¿…é¡»åŒæ—¶æ»¡è¶³ä¸‰ä¸ªç¡¬æŒ‡æ ‡
        if (cand['bert'] >= THRESHOLD_BERT and 
            cand['comet'] >= THRESHOLD_COMET and 
            cand['visual'] >= THRESHOLD_CLIP):
            qualified.append(cand)

    if not qualified:
        return None, None, None

    # 3. å†³èµ› (COMET å†³èƒœè´Ÿ)
    # æŒ‰ COMET åˆ†æ•°ä»é«˜åˆ°ä½æ’åºï¼Œå–ç¬¬ä¸€ä¸ª
    best_cand = sorted(qualified, key=lambda x: x['comet'], reverse=True)[0]
    
    return best_cand['text'], best_cand['model'], {
        "bert": best_cand['bert'],
        "comet": best_cand['comet'],
        "visual": best_cand['visual']
    }

def process_dataset(data, output_file):
    print("ğŸš€ å¼€å§‹æ‰§è¡Œ [ä¼˜ä¸­é€‰ä¼˜] ç­–ç•¥...")
    
    final_data = []
    stats = {
        "total": len(data),
        "kept": 0,
        "lang_stats": defaultdict(lambda: {"short_nllb":0, "short_seamless":0, "long_nllb":0, "long_seamless":0})
    }

    for item in data:
        if 'translations' not in item: continue
        
        new_translations = {}
        has_content = False
        
        # éå†æ¯ç§è¯­è¨€
        for lang, trans_obj in item['translations'].items():
            lang_result = {}
            
            # --- å¤„ç† Short Caption ---
            s_text, s_model, s_scores = select_best_candidate(trans_obj, "short")
            if s_text:
                lang_result["short_translation"] = s_text
                lang_result["short_model"] = s_model # è®°å½•æ˜¯è°èµ¢äº†
                lang_result["short_scores"] = s_scores
                stats["lang_stats"][lang][f"short_{s_model}"] += 1
            
            # --- å¤„ç† Long Caption ---
            l_text, l_model, l_scores = select_best_candidate(trans_obj, "long")
            if l_text:
                lang_result["long_translation"] = l_text
                lang_result["long_model"] = l_model
                lang_result["long_scores"] = l_scores
                stats["lang_stats"][lang][f"long_{l_model}"] += 1
            
            # åªæœ‰å½“è¯¥è¯­è¨€è‡³å°‘ä¿ç•™äº†ä¸€ä¸ª caption æ—¶ï¼Œæ‰å†™å…¥
            if lang_result:
                new_translations[lang] = lang_result
                has_content = True

        if has_content:
            # æ„å»ºæå…¶å¹²å‡€çš„æœ€ç»ˆæ•°æ®ç»“æ„
            final_item = {
                "image_id": item['image_id'],
                "path": item['path'],
                # æºè‹±æ–‡
                "src_short": item.get('short_caption_best'),
                "src_long": item.get('long_caption_best'),
                # ç­›é€‰åçš„å¤šè¯­ç§ç¿»è¯‘
                "translations": new_translations
            }
            final_data.append(final_item)
            stats["kept"] += 1

    # ä¿å­˜
    print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆé»„é‡‘æ•°æ®é›†è‡³: {output_file}")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)

    # æ‰“å°æˆ˜æŠ¥
    print("\n" + "="*50)
    print("ğŸ† FINAL DATASET STATISTICS")
    print("="*50)
    print(f"åŸå§‹æ•°æ® : {stats['total']}")
    print(f"æœ€ç»ˆä¿ç•™ : {stats['kept']} (ä¿ç•™ç‡: {stats['kept']/stats['total']*100:.2f}%)")
    print("-" * 50)
    print("å„è¯­è¨€æ¨¡å‹èƒœå‡ºåˆ†å¸ƒ (Winning Model Distribution):")
    print(f"{'Language':<12} | {'Short NLLB':<10} {'Seamless':<10} | {'Long NLLB':<10} {'Seamless':<10}")
    print("-" * 60)
    
    for lang, counts in stats["lang_stats"].items():
        s_n = counts['short_nllb']
        s_s = counts['short_seamless']
        l_n = counts['long_nllb']
        l_s = counts['long_seamless']
        print(f"{lang:<12} | {s_n:<10} {s_s:<10} | {l_n:<10} {l_s:<10}")
        
    print("="*50)
    print("âœ¨ æ•°æ®é›†æ„å»ºå®Œæˆã€‚è¿™æ˜¯ç»å¯¹çš„æœ€ä¼˜è§£ã€‚")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # è¾“å…¥å¿…é¡»æ˜¯ä¸Šä¸€æ­¥å¸¦æœ‰åˆ†æ•°çš„ json
    parser.add_argument("--input_file", type=str, default="dataset_scored_final.json")
    parser.add_argument("--output_file", type=str, default="dataset_golden.json")
    args = parser.parse_args()

    data = load_data(args.input_file)
    process_dataset(data, args.output_file)
    
    
    
"""
python 06_filter_final.py \
  --input_file "dataset_scored_final.json" \
  --output_file "dataset_optimal_filtered.json"
"""


"""

æ ¸å¿ƒç®—æ³•æµç¨‹ï¼š
1ã€èµ„æ ¼èµ›ï¼ˆHard Filteringï¼‰ï¼š
    é¦–å…ˆæ£€æŸ¥ NLLB å’Œ Seamless çš„ç¿»è¯‘æ˜¯å¦éƒ½è¾¾åˆ°äº†**â€œåŠæ ¼çº¿â€**ï¼ˆå³ä¸Šä¸€è½®è®¾å®šçš„ BERT>0.88, CLIP>0.22, COMET>0.72ï¼‰ã€‚
    å¦‚æœæŸä¸€ä¸ªæ¨¡å‹æ²¡åŠæ ¼ï¼Œç›´æ¥æ·˜æ±°ã€‚
    å¦‚æœä¸¤ä¸ªéƒ½æ²¡åŠæ ¼ï¼Œè¿™æ¡æ•°æ®å¯¹åº”çš„ç¿»è¯‘ä»»åŠ¡ï¼ˆShort æˆ– Longï¼‰ç›´æ¥åºŸå¼ƒã€‚
2ã€å†³èµ›ï¼ˆWinner Selectionï¼‰ï¼š
    å¦‚æœä¸¤ä¸ªæ¨¡å‹éƒ½åŠæ ¼äº†ï¼Œè°æ›´å¥½ï¼Ÿ
    åˆ¤å†³æ ‡å‡†ï¼šæ¯”è¾ƒ COMET-Kiwi åˆ†æ•°ã€‚
    ç†ç”±ï¼šåœ¨éƒ½ä¿è¯äº†è¯­ä¹‰ï¼ˆBERTï¼‰å’Œè§†è§‰ï¼ˆCLIPï¼‰æ­£ç¡®çš„å‰æä¸‹ï¼ŒCOMET-Kiwi åˆ†æ•°è¶Šé«˜ï¼Œä»£è¡¨è¯‘æ–‡è¶Šåœ°é“ã€è¶Šç¬¦åˆäººç±»é˜…è¯»ä¹ æƒ¯ã€‚æˆ‘ä»¬é€‰å– COMET åˆ†æ•°æ›´é«˜çš„é‚£ä¸ªä½œä¸ºæœ€ç»ˆç»“æœã€‚
3ã€ç»´å¾å°”è¯­ç‰¹ä¾‹ï¼š
    ç”±äºåªæœ‰ NLLBï¼Œå®ƒç›´æ¥è¿›å…¥â€œèµ„æ ¼èµ›â€ã€‚åŠæ ¼å°±ç•™ï¼Œä¸åŠæ ¼å°±æ‰”ã€‚
"""