import json
import sys
import argparse
import csv
from collections import defaultdict
import numpy as np

# ==========================================
# ğŸ”¥ [ä¸¥é€‰é˜ˆå€¼é…ç½®] (ä¿æŒåŸå§‹ 0-1 æ ¼å¼ç”¨äºæ¯”å¯¹) ğŸ”¥
# ==========================================
THRESHOLD_BERT = 0.90   
THRESHOLD_COMET = 0.78  
THRESHOLD_CLIP = 0.27   

# è¯­è¨€ä»£ç æ˜ å°„
LANG_MAP = {
    'uyghur': 'ug', 'ug': 'ug',
    'kazakh': 'kk', 'kk': 'kk',
    'kirghiz': 'ky', 'kyrgyz': 'ky', 'ky': 'ky',
    'tajik': 'tg', 'tg': 'tg',
    'urdu': 'ur', 'ur': 'ur',
    'uzbek': 'uz', 'uz': 'uz'
}
TARGET_LANGS = ['ug', 'uz', 'kk', 'ky', 'tg', 'ur']

def load_data(file_path):
    print(f"ğŸ“– è¯»å–æ•°æ®: {file_path} ...")
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            first_char = f.read(1)
            f.seek(0)
            if first_char == '[':
                return json.load(f)
            else:
                return [json.loads(line) for line in f if line.strip()]
    except Exception as e:
        print(f"âŒ è¯»å–å¤±è´¥: {e}"); sys.exit(1)

def check_entry_quality(lang_data, type_prefix, target_model):
    """
    æ£€æŸ¥å•æ¡è¯­è¨€æ•°æ®ä¸­ï¼Œç‰¹å®šç±»å‹çš„ç¿»è¯‘æ˜¯å¦ç¬¦åˆè¦æ±‚
    """
    model_key = f"{type_prefix}_model"
    score_key = f"{type_prefix}_scores"
    text_key = f"{type_prefix}_translation"

    # 1. æ£€æŸ¥æ˜¯å¦å­˜åœ¨è¯¥ç±»å‹çš„æ¨¡å‹è®°å½•
    if model_key not in lang_data or not lang_data[model_key]:
        return None
    
    # 2. æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦åŒ¹é…
    current_model = lang_data[model_key].lower()
    if current_model != target_model.lower():
        return None

    # 3. è·å–åˆ†æ•° (åŸå§‹ 0-1 åˆ†æ•°)
    scores = lang_data.get(score_key, {})
    if not scores: return None
    
    bert = scores.get('bert', -1)
    comet = scores.get('comet', -1)
    visual = scores.get('visual', -1)

    # 4. é˜ˆå€¼è¿‡æ»¤ (ä½¿ç”¨åŸå§‹ 0-1 é˜ˆå€¼è¿›è¡Œåˆ¤æ–­)
    if (bert >= THRESHOLD_BERT and 
        comet >= THRESHOLD_COMET and 
        visual >= THRESHOLD_CLIP):
        
        # è¿”å›ç»“æœæ—¶ï¼Œå°†åˆ†æ•°è½¬æ¢ä¸ºç™¾åˆ†åˆ¶ (0-100)
        return {
            "type": type_prefix,
            "text": lang_data[text_key],
            "scores": {
                "bert": bert * 100,
                "comet": comet * 100,
                "visual": visual * 100
            }
        }
    
    return None

def process_dataset(data, target_model, output_json, output_csv):
    print(f"ğŸš€ å¼€å§‹ç­›é€‰ï¼Œç›®æ ‡æ¨¡å‹: [{target_model.upper()}]")
    print(f"ğŸ¯ åŸå§‹é˜ˆå€¼è®¾å®š: BERT>={THRESHOLD_BERT}, COMET>={THRESHOLD_COMET}, CLIP>={THRESHOLD_CLIP}")
    print(f"ğŸ“ è¾“å‡ºç»“æœå°†è½¬æ¢ä¸ºç™¾åˆ†åˆ¶ (0-100)")
    
    final_data = []
    
    # åˆ†è¯­è¨€ç»Ÿè®¡å™¨
    stats = defaultdict(lambda: {"bert": [], "comet": [], "visual": [], "count": 0})
    # ğŸ”¥ å…¨å±€ç»Ÿè®¡å™¨ (ç”¨äºè®¡ç®— Total Average)
    global_stats = {"bert": [], "comet": [], "visual": [], "count": 0}
    
    csv_rows = []

    for item in data:
        if 'translations' not in item: continue
        
        new_translations = {}
        has_content = False
        
        for lang_name, lang_content in item['translations'].items():
            lang_code = LANG_MAP.get(lang_name.lower(), lang_name)
            lang_result = {}
            
            # --- 1. æ£€æŸ¥ Short Translation ---
            res_short = check_entry_quality(lang_content, "short", target_model)
            if res_short:
                lang_result["short_translation"] = res_short['text']
                lang_result["short_model"] = target_model
                lang_result["short_scores"] = res_short['scores']
                
                s = res_short['scores']
                # è®°å½•åˆ†è¯­è¨€ç»Ÿè®¡
                stats[lang_code]["bert"].append(s['bert'])
                stats[lang_code]["comet"].append(s['comet'])
                stats[lang_code]["visual"].append(s['visual'])
                stats[lang_code]["count"] += 1
                
                # ğŸ”¥ è®°å½•å…¨å±€ç»Ÿè®¡
                global_stats["bert"].append(s['bert'])
                global_stats["comet"].append(s['comet'])
                global_stats["visual"].append(s['visual'])
                global_stats["count"] += 1
                
                csv_rows.append([
                    item.get('image_id', 'N/A'), lang_code, "Short", target_model,
                    f"{s['bert']:.2f}", f"{s['comet']:.2f}", f"{s['visual']:.2f}",
                    res_short['text']
                ])

            # --- 2. æ£€æŸ¥ Long Translation ---
            res_long = check_entry_quality(lang_content, "long", target_model)
            if res_long:
                lang_result["long_translation"] = res_long['text']
                lang_result["long_model"] = target_model
                lang_result["long_scores"] = res_long['scores']
                
                s = res_long['scores']
                # è®°å½•åˆ†è¯­è¨€ç»Ÿè®¡
                stats[lang_code]["bert"].append(s['bert'])
                stats[lang_code]["comet"].append(s['comet'])
                stats[lang_code]["visual"].append(s['visual'])
                stats[lang_code]["count"] += 1

                # ğŸ”¥ è®°å½•å…¨å±€ç»Ÿè®¡
                global_stats["bert"].append(s['bert'])
                global_stats["comet"].append(s['comet'])
                global_stats["visual"].append(s['visual'])
                global_stats["count"] += 1
                
                csv_rows.append([
                    item.get('image_id', 'N/A'), lang_code, "Long", target_model,
                    f"{s['bert']:.2f}", f"{s['comet']:.2f}", f"{s['visual']:.2f}",
                    res_long['text']
                ])

            if lang_result:
                new_translations[lang_name] = lang_result
                has_content = True

        if has_content:
            final_item = item.copy()
            final_item['translations'] = new_translations
            final_data.append(final_item)

    # --- ä¿å­˜ç»“æœ ---
    print(f"\nğŸ’¾ ä¿å­˜è¿‡æ»¤åçš„ JSON (ç™¾åˆ†åˆ¶): {output_json}")
    with open(output_json, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“Š ä¿å­˜ CSV æŠ¥è¡¨ (ç™¾åˆ†åˆ¶): {output_csv}")
    with open(output_csv, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(["Image_ID", "Language", "Type", "Model", "BERT", "COMET", "CLIP", "Text"])
        writer.writerows(csv_rows)

    # --- æ‰“å°ç»ˆç«¯æŠ¥è¡¨ ---
    print("\n" + "="*90)
    print(f"ğŸ† æ¨¡å‹ [{target_model.upper()}] è´¨é‡åˆ†ææŠ¥å‘Š (Score: 0-100)")
    print("="*90)
    
    header = "{:<10} | {:<10} | {:<10} | {:<10} | {:<10}".format("Language", "Count", "Avg BERT", "Avg COMET", "Avg CLIP")
    print(header)
    print("-" * 90)

    existing_langs = sorted(list(stats.keys()))
    sorted_langs = [l for l in TARGET_LANGS if l in existing_langs] + [l for l in existing_langs if l not in TARGET_LANGS]

    for lang in sorted_langs:
        st = stats[lang]
        count = st['count']
        
        if count > 0:
            avg_bert = np.mean(st['bert'])
            avg_comet = np.mean(st['comet'])
            avg_clip = np.mean(st['visual'])
            print("{:<10} | {:<10} | {:<10.2f} | {:<10.2f} | {:<10.2f}".format(
                lang, count, avg_bert, avg_comet, avg_clip
            ))
        else:
            print("{:<10} | {:<10} | -          | -          | -".format(lang, 0))

    # ğŸ”¥ æ‰“å°å…¨å±€å¹³å‡è¡Œ
    print("-" * 90)
    if global_stats["count"] > 0:
        g_bert = np.mean(global_stats["bert"])
        g_comet = np.mean(global_stats["comet"])
        g_clip = np.mean(global_stats["visual"])
        print("{:<10} | {:<10} | {:<10.2f} | {:<10.2f} | {:<10.2f}".format(
            "AVERAGE", global_stats["count"], g_bert, g_comet, g_clip
        ))
    else:
        print("{:<10} | {:<10} | -          | -          | -".format("AVERAGE", 0))

    print("="*90)
    print(f"åŸå§‹å›¾ç‰‡æ€»æ•°: {len(data)}")
    print(f"åŒ…å«æœ‰æ•ˆæ•°æ®çš„å›¾ç‰‡æ•°: {len(final_data)}")
    print(f"æ€»è®¡ä¿ç•™æ¡ç›®æ•° (Short + Long): {global_stats['count']}")
    print("="*90)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_file", type=str, required=True, help="è¾“å…¥ JSON æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model", type=str, required=True, help="è¦ç­›é€‰çš„æ¨¡å‹åç§° (å¦‚: nllb, seamless)")
    parser.add_argument("--output_dir", type=str, default=".", help="è¾“å‡ºç›®å½•")
    args = parser.parse_args()

    out_json = f"{args.output_dir}/filtered_{args.model}.json"
    out_csv = f"{args.output_dir}/report_{args.model}.csv"

    data = load_data(args.input_file)
    process_dataset(data, args.model, out_json, out_csv)
