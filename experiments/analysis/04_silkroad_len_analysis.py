import json
import os
import argparse
from collections import defaultdict
from tqdm import tqdm

# ==========================================
# ğŸ› ï¸ é»˜è®¤é…ç½®
# ==========================================
DEFAULT_INPUT_FILE = "dataset_optimal_filtered_v3.json"
DEFAULT_OUTPUT_DIR = "final_datasets_split"

def load_data(file_path):
    print(f"ğŸ“– æ­£åœ¨è¯»å–æ•°æ®é›†: {file_path} ...")
    if not os.path.exists(file_path):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {file_path}")
        exit(1)
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            # å…¼å®¹ JSON æ•°ç»„æˆ– JSONL
            first_char = f.read(1)
            f.seek(0)
            if first_char == '[':
                return json.load(f)
            else:
                return [json.loads(line) for line in f if line.strip()]
    except Exception as e:
        print(f"âŒ è¯»å–å¤±è´¥: {e}")
        exit(1)

def split_by_language_and_length(data, output_dir):
    # 1. åˆ›å»ºè¾“å‡ºç›®å½•
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"ğŸ“‚ å·²åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")

    # 2. åˆå§‹åŒ–ç¼“å­˜å®¹å™¨
    # ç»“æ„: buffers[lang]['short'] = List[Dict]
    lang_buffers = defaultdict(lambda: {'short': [], 'long': []})
    
    # 3. åˆå§‹åŒ–é•¿åº¦ç»Ÿè®¡ (ç”¨äºè®¡ç®—å¹³å‡é•¿åº¦)
    # ç»“æ„: len_stats[lang]['short'] = total_word_count
    len_stats = defaultdict(lambda: {'short': 0, 'long': 0})
    
    print("ğŸš€ æ­£åœ¨æ‰§è¡Œ [è¯­è¨€ + é•¿çŸ­å¥] äºŒçº§æ‹†åˆ†åŠç»Ÿè®¡...")
    
    # 4. éå†æ•°æ®
    for item in tqdm(data):
        if 'translations' not in item: continue
        
        # åŸºç¡€å…ƒæ•°æ®
        base_meta = {
            "image_id": item.get("image_id"),
            "path": item.get("path"),
        }

        # éå†è¯¥æ¡ç›®ä¸‹çš„æ‰€æœ‰è¯­è¨€
        for lang, trans_content in item['translations'].items():
            if not trans_content: continue

            # --- å¤„ç† Short Caption ---
            tgt_short = trans_content.get("short_translation")
            if tgt_short:
                short_entry = base_meta.copy()
                short_entry.update({
                    "type": "short",
                    "src_text": item.get("src_short"),
                    "tgt_text": tgt_short,
                    "model": trans_content.get("short_model"),
                    "scores": trans_content.get("short_scores")
                })
                lang_buffers[lang]['short'].append(short_entry)
                
                # ğŸ“Š ç»Ÿè®¡é•¿åº¦ (æŒ‰ç©ºæ ¼åˆ†å‰²è®¡ç®—è¯æ•°ï¼Œä¸­æ–‡ç­‰æ— ç©ºæ ¼è¯­è¨€å¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œæ­¤å¤„ä¸ºé€šç”¨è¿‘ä¼¼)
                len_stats[lang]['short'] += len(tgt_short.split())

            # --- å¤„ç† Long Caption ---
            tgt_long = trans_content.get("long_translation")
            if tgt_long:
                long_entry = base_meta.copy()
                long_entry.update({
                    "type": "long",
                    "src_text": item.get("src_long"),
                    "tgt_text": tgt_long,
                    "model": trans_content.get("long_model"),
                    "scores": trans_content.get("long_scores")
                })
                lang_buffers[lang]['long'].append(long_entry)
                
                # ğŸ“Š ç»Ÿè®¡é•¿åº¦
                len_stats[lang]['long'] += len(tgt_long.split())

    # 5. ä¿å­˜æ–‡ä»¶å¹¶æ‰“å°ç»Ÿè®¡è¡¨
    print("\nğŸ’¾ ä¿å­˜ç»“æœç»Ÿè®¡:")
    print("=" * 85)
    # è°ƒæ•´è¡¨å¤´ï¼Œå¢åŠ  Avg Len åˆ—
    print(f"{'Language':<12} | {'Type':<6} | {'Count':<8} | {'Avg Len':<8} | {'Output Filename'}")
    print("-" * 85)

    if not lang_buffers:
        print("âš ï¸ è­¦å‘Š: æ²¡æœ‰æå–åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼")
        return

    # æŒ‰è¯­è¨€å­—æ¯é¡ºåºæ’åºè¾“å‡º
    for lang in sorted(lang_buffers.keys()):
        types = lang_buffers[lang]
        
        # --- ä¿å­˜ Short æ–‡ä»¶ ---
        if types['short']:
            count = len(types['short'])
            # è®¡ç®—å¹³å‡é•¿åº¦
            avg_len = len_stats[lang]['short'] / count if count > 0 else 0
            
            filename_s = f"{lang}_short.json"
            path_s = os.path.join(output_dir, filename_s)
            
            with open(path_s, 'w', encoding='utf-8') as f:
                json.dump(types['short'], f, ensure_ascii=False, indent=2)
            
            print(f"{lang:<12} | {'Short':<6} | {count:<8} | {avg_len:<8.1f} | {filename_s}")

        # --- ä¿å­˜ Long æ–‡ä»¶ ---
        if types['long']:
            count = len(types['long'])
            # è®¡ç®—å¹³å‡é•¿åº¦
            avg_len = len_stats[lang]['long'] / count if count > 0 else 0
            
            filename_l = f"{lang}_long.json"
            path_l = os.path.join(output_dir, filename_l)
            
            with open(path_l, 'w', encoding='utf-8') as f:
                json.dump(types['long'], f, ensure_ascii=False, indent=2)
            
            print(f"{lang:<12} | {'Long':<6} | {count:<8} | {avg_len:<8.1f} | {filename_l}")

    print("=" * 85)
    print(f"ğŸ‰ æ‹†åˆ†å®Œæˆï¼æ–‡ä»¶ä½äº '{output_dir}/' ç›®å½•ä¸‹ã€‚")
    print(f"ğŸ’¡ æ³¨: 'Avg Len' æ˜¯åŸºäºç©ºæ ¼åˆ†å‰²çš„è¿‘ä¼¼è¯æ•°ç»Ÿè®¡ã€‚")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Split dataset and calc stats.")
    parser.add_argument("--input_file", type=str, default=DEFAULT_INPUT_FILE)
    parser.add_argument("--output_dir", type=str, default=DEFAULT_OUTPUT_DIR)
    args = parser.parse_args()

    dataset = load_data(args.input_file)
    split_by_language_and_length(dataset, args.output_dir)
