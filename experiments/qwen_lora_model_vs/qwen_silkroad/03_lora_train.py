import os

# âœ… 1. æ˜¾å­˜ç¢ç‰‡ä¼˜åŒ– (ä¿æŒä¸å˜)
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

import json
import torch
from dataclasses import dataclass, field
from typing import Dict, Optional, List
from PIL import Image

import transformers
from transformers import (
    AutoModelForImageTextToText,  # ä½¿ç”¨æ–°ç‰ˆç±»å
    AutoProcessor,
    Trainer,
    TrainingArguments
)
# âœ… 2. å¼•å…¥æ£€æŸ¥ç‚¹å·¥å…·
from transformers.trainer_utils import get_last_checkpoint

from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType
)

# ================= ğŸ”´ æ ¸å¿ƒé…ç½®åŒºåŸŸ ğŸ”´ =================
MODEL_ID = "models/Qwen3-VL-8B-Instruct"
DATA_PATH = "outputs/silkroad_train.json"
OUTPUT_DIR = "outputs/qwen_native_output"

# H100 é…ç½®
BATCH_SIZE = 8
GRAD_ACCUM = 8
NUM_EPOCHS = 3
LEARNING_RATE = 1e-4
MAX_LEN = 2048

# åˆ†è¾¨ç‡é™åˆ¶
MIN_PIXELS = 256 * 28 * 28
MAX_PIXELS = 1024 * 28 * 28

# âœ… 3. æ–°å¢ï¼šä¿å­˜ç­–ç•¥ (é˜²æ­¢è®­ç»ƒå‡ å°æ—¶åå´©æºƒç™½è·‘)
SAVE_STEPS = 500  # æ¯è·‘ 500 æ­¥å­˜ä¸€æ¬¡æ¡£
SAVE_TOTAL_LIMIT = 2  # åªä¿ç•™æœ€è¿‘çš„ 2 ä¸ªå­˜æ¡£ï¼ŒèŠ‚çœç¡¬ç›˜ç©ºé—´


# =======================================================

def load_data(data_path):
    with open(data_path, 'r', encoding='utf-8') as f:
        return json.load(f)


# âœ… 4. è‡ªå®šä¹‰ Dataset (ä¿æŒä¸å˜)
class QwenVLDataset(torch.utils.data.Dataset):
    def __init__(self, data, processor):
        self.data = data
        self.processor = processor

    def __len__(self):
        return len(self.data)

    def __getitem__(self, i):
        item = self.data[i]
        image_path = item['images'][0]
        conversations = item['conversations']

        try:
            image = Image.open(image_path).convert("RGB")
        except Exception as e:
            print(f"âŒ Bad Image: {image_path} | {e}")
            image = Image.new('RGB', (224, 224), (0, 0, 0))

        user_text = conversations[0]['value'].replace('<image>', '').strip()
        assistant_text = conversations[1]['value']

        messages_full = [
            {"role": "user", "content": [{"type": "image"}, {"type": "text", "text": user_text}]},
            {"role": "assistant", "content": [{"type": "text", "text": assistant_text}]}
        ]
        messages_prompt = [
            {"role": "user", "content": [{"type": "image"}, {"type": "text", "text": user_text}]}
        ]

        text_full = self.processor.apply_chat_template(messages_full, tokenize=False, add_generation_prompt=False)
        text_prompt = self.processor.apply_chat_template(messages_prompt, tokenize=False, add_generation_prompt=True)

        inputs = self.processor(text=[text_full], images=[image], videos=None, padding=False, return_tensors="pt")
        inputs_prompt = self.processor(text=[text_prompt], images=[image], padding=False, return_tensors="pt")

        input_ids = inputs["input_ids"][0]
        labels = input_ids.clone()
        prompt_len = inputs_prompt["input_ids"].shape[1]

        if prompt_len < len(labels):
            labels[:prompt_len] = -100
        else:
            labels[:len(labels) - 1] = -100

        return {
            "input_ids": input_ids,
            "attention_mask": inputs["attention_mask"][0],
            "pixel_values": inputs["pixel_values"],
            "image_grid_thw": inputs["image_grid_thw"][0],
            "labels": labels
        }


# âœ… 5. è‡ªå®šä¹‰ Collator (ä¿æŒä¸å˜)
@dataclass
class DataCollatorForQwenVL:
    processor: AutoProcessor

    def __call__(self, features):
        input_ids = [f["input_ids"] for f in features]
        attention_mask = [f["attention_mask"] for f in features]
        pixel_values = [f["pixel_values"] for f in features]
        image_grid_thw = [f["image_grid_thw"] for f in features]
        labels = [f["labels"] for f in features]

        input_ids_padded = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True,
                                                           padding_value=self.processor.tokenizer.pad_token_id)
        attention_mask_padded = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)
        labels_padded = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)

        pixel_values_cat = torch.cat(pixel_values, dim=0)
        image_grid_thw_cat = torch.stack(image_grid_thw, dim=0)

        return {
            "input_ids": input_ids_padded,
            "attention_mask": attention_mask_padded,
            "pixel_values": pixel_values_cat,
            "image_grid_thw": image_grid_thw_cat,
            "labels": labels_padded
        }


# âœ… 6. æ ¸å¿ƒä¿®æ”¹ï¼šé˜²å´©æºƒ Trainer
class RobustTrainer(Trainer):
    """
    ä¸€ä¸ªå¼ºå£®çš„ Trainerï¼Œé‡åˆ° OOM é”™è¯¯æ—¶ä¸ä¼šå´©æºƒï¼Œè€Œæ˜¯è·³è¿‡è¯¥ batch ç»§ç»­è®­ç»ƒã€‚
    """

    def training_step(self, model, inputs, num_items_in_batch=None):
        try:
            # å°è¯•æ­£å¸¸æ‰§è¡Œè®­ç»ƒæ­¥
            return super().training_step(model, inputs, num_items_in_batch)
        except torch.cuda.OutOfMemoryError:
            # æ•è· OOM é”™è¯¯
            torch.cuda.empty_cache()
            print(
                f"\nâš ï¸ [OOM Warning] Step {self.state.global_step}: GPU Out of Memory! Skipping this batch to continue training...")
            # è¿”å›ä¸€ä¸ª 0 æŸå¤±ï¼Œä¸å½±å“æ¢¯åº¦ï¼Œä½†èƒ½ä¿æŒè®­ç»ƒå¾ªç¯ä¸ä¸­æ–­
            return torch.tensor(0.0, device=model.device, requires_grad=True)


def train():
    print("â³ Loading Processor...")
    processor = AutoProcessor.from_pretrained(
        MODEL_ID, trust_remote_code=True, min_pixels=MIN_PIXELS, max_pixels=MAX_PIXELS
    )

    print("â³ Loading Model...")
    model = AutoModelForImageTextToText.from_pretrained(
        MODEL_ID,
        torch_dtype=torch.bfloat16,
        quantization_config=transformers.BitsAndBytesConfig(
            load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        ),
        attn_implementation="sdpa",
        device_map="auto",
        trust_remote_code=True
    )

    model = prepare_model_for_kbit_training(model)
    peft_config = LoraConfig(
        r=64, lora_alpha=128, target_modules="all-linear",
        lora_dropout=0.05, bias="none", task_type="CAUSAL_LM", use_dora=True,
    )
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    print("â³ Processing Data...")
    raw_data = load_data(DATA_PATH)
    dataset = QwenVLDataset(raw_data, processor)
    collator = DataCollatorForQwenVL(processor)

    # âœ… 7. è‡ªåŠ¨æ£€æŸ¥æ˜¯å¦å­˜åœ¨ Checkpoint
    last_checkpoint = None
    if os.path.isdir(OUTPUT_DIR):
        last_checkpoint = get_last_checkpoint(OUTPUT_DIR)

    if last_checkpoint:
        print(f"ğŸ”„ å‘ç°å­˜æ¡£ï¼Œå°†ä»æ–­ç‚¹ç»§ç»­è®­ç»ƒ: {last_checkpoint}")
    else:
        print("ğŸš€ æœªå‘ç°å­˜æ¡£ï¼Œå¼€å§‹æ–°è®­ç»ƒ...")

    args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRAD_ACCUM,
        learning_rate=LEARNING_RATE,
        num_train_epochs=NUM_EPOCHS,
        lr_scheduler_type="cosine",
        warmup_ratio=0.03,
        optim="adamw_torch_fused",
        bf16=True,
        logging_steps=5,
        dataloader_num_workers=16,
        dataloader_pin_memory=True,
        report_to="none",
        remove_unused_columns=False,

        # âœ… 8. ä¿®æ”¹ä¿å­˜ç­–ç•¥ï¼šæŒ‰æ­¥æ•°ä¿å­˜
        save_strategy="steps",
        save_steps=SAVE_STEPS,
        save_total_limit=SAVE_TOTAL_LIMIT,  # åªä¿ç•™æœ€è¿‘2ä¸ªï¼Œé˜²æ­¢ç¡¬ç›˜çˆ†æ»¡

        # ç¡®ä¿ç»§ç»­è®­ç»ƒæ—¶èƒ½åŠ è½½ä¹‹å‰çš„çŠ¶æ€
        overwrite_output_dir=False,
    )

    # âœ… 9. ä½¿ç”¨è‡ªå®šä¹‰çš„ RobustTrainer
    trainer = RobustTrainer(
        model=model,
        args=args,
        train_dataset=dataset,
        data_collator=collator,
    )

    print("ğŸš€ Starting Robust Training...")

    # âœ… 10. å¯åŠ¨è®­ç»ƒ (ä¼ å…¥æ–­ç‚¹è·¯å¾„)
    trainer.train(resume_from_checkpoint=last_checkpoint)

    print("ğŸ’¾ Saving Final Model...")
    trainer.save_model(OUTPUT_DIR)
    processor.save_pretrained(OUTPUT_DIR)


if __name__ == "__main__":
    train()
