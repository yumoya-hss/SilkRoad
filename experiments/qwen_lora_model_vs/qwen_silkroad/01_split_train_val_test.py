import os
import json
import random
import shutil

# ================= ğŸ”´ é…ç½® =================
SOURCE_DIR = "outputs/final_datasets_split"
OUTPUT_ROOT = "outputs/split"
TEST_SAMPLES_PER_LANG = 250
VAL_SAMPLES_PER_LANG = 50

# ğŸ”’ éšæœºç§å­
SEED = 42
# ===========================================

def split_dataset():
    # 1. é”å®šéšæœºç§å­
    random.seed(SEED)
    print(f"ğŸ”’ éšæœºç§å­å·²é”å®šä¸º: {SEED} (ä¿è¯æ¯æ¬¡åˆ’åˆ†ç»“æœä¸€è‡´)")

    for sub in ['train', 'val', 'test']:
        path = os.path.join(OUTPUT_ROOT, sub)
        os.makedirs(path, exist_ok=True)

    # 2. è¯»å–å¹¶æ’åºæ–‡ä»¶
    files = [f for f in os.listdir(SOURCE_DIR) if f.endswith('.json')]
    files.sort()

    lang_files = {}
    for f in files:
        # ğŸ”´ ä¿®æ”¹æ ¸å¿ƒï¼šé’ˆå¯¹ dataset_kazakh_long.json è¿™ç§æ–‡ä»¶å
        # split('_') å¾—åˆ° ['dataset', 'kazakh', 'long.json']
        # æˆ‘ä»¬å– [1] ä¹Ÿå°±æ˜¯ 'kazakh'
        try:
            lang = f.split('_')[1].lower()
        except IndexError:
            # é˜²æ­¢ä¸‡ä¸€æœ‰ä¸ªæ–‡ä»¶å« data.json è¿™ç§æ²¡æœ‰ä¸‹åˆ’çº¿çš„ï¼Œåšä¸ªä¿åº•
            lang = f.split('.')[0].lower()

        if lang not in lang_files: lang_files[lang] = []
        lang_files[lang].append(f)

    print(f"ğŸŒ è¯†åˆ«åˆ° {len(lang_files)} ç§è¯­è¨€: {list(lang_files.keys())}")

    # 3. å¼€å§‹åˆ†å±‚å¤„ç†
    for lang, file_list in lang_files.items():
        all_items = []
        for fname in file_list:
            with open(os.path.join(SOURCE_DIR, fname), 'r', encoding='utf-8') as f:
                all_items.extend(json.load(f))

        grouped_by_img = {}
        for item in all_items:
            img_path = item.get('path')
            if img_path not in grouped_by_img: grouped_by_img[img_path] = []
            grouped_by_img[img_path].append(item)

        # æ’åºå†æ‰“ä¹±
        unique_images = list(grouped_by_img.keys())
        unique_images.sort()
        random.shuffle(unique_images)

        total_imgs = len(unique_images)
        print(f"   Processing {lang}: æ€»å›¾ç‰‡æ•° {total_imgs} ...")

        n_test_imgs = TEST_SAMPLES_PER_LANG
        n_val_imgs = VAL_SAMPLES_PER_LANG

        if total_imgs < (n_test_imgs + n_val_imgs) * 2:
            print(f"   âš ï¸ è­¦å‘Š: {lang} æ•°æ®è¿‡å°‘ï¼Œåˆ‡æ¢ä¸º 10% æµ‹è¯•é›†æ¨¡å¼")
            n_test_imgs = int(total_imgs * 0.1)
            n_val_imgs = int(total_imgs * 0.05)

        test_img_ids = unique_images[:n_test_imgs]
        val_img_ids = unique_images[n_test_imgs: n_test_imgs + n_val_imgs]
        train_img_ids = unique_images[n_test_imgs + n_val_imgs:]

        def flatten_data(img_ids):
            data = []
            for img_id in img_ids:
                data.extend(grouped_by_img[img_id])
            return data

        test_data = flatten_data(test_img_ids)
        val_data = flatten_data(val_img_ids)
        train_data = flatten_data(train_img_ids)

        with open(os.path.join(OUTPUT_ROOT, 'test', f'{lang}.json'), 'w', encoding='utf-8') as f:
            json.dump(test_data, f, ensure_ascii=False, indent=2)

        with open(os.path.join(OUTPUT_ROOT, 'val', f'{lang}.json'), 'w', encoding='utf-8') as f:
            json.dump(val_data, f, ensure_ascii=False, indent=2)

        with open(os.path.join(OUTPUT_ROOT, 'train', f'{lang}.json'), 'w', encoding='utf-8') as f:
            json.dump(train_data, f, ensure_ascii=False, indent=2)

        print(f"     -> Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_data)}")

    print("\nâœ… æ‰€æœ‰è¯­è¨€åˆ†å±‚åˆ‡åˆ†å®Œæˆï¼")

if __name__ == "__main__":
    split_dataset()
