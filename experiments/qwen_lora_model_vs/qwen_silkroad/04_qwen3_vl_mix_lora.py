import torch
from transformers import AutoModelForImageTextToText, AutoProcessor
from peft import PeftModel
import os

# ================= é…ç½® =================
BASE_MODEL_PATH = "models/Qwen3-VL-8B-Instruct"
ADAPTER_PATH = "outputs/qwen_native_output"
SAVE_PATH = "models/SilkRoad-MMT-8B"
# =======================================

print(f"â³ Loading Base Model from {BASE_MODEL_PATH}...")
# å»ºè®®ä¿®æ”¹ï¼šä½¿ç”¨ bfloat16 ä¸è®­ç»ƒä¿æŒä¸€è‡´
model = AutoModelForImageTextToText.from_pretrained(
    BASE_MODEL_PATH,
    torch_dtype=torch.bfloat16,  # âœ… æ”¹ä¸º bfloat16
    device_map="auto",
    trust_remote_code=True
)

print(f"â³ Loading LoRA Adapter from {ADAPTER_PATH}...")
model = PeftModel.from_pretrained(model, ADAPTER_PATH)

print("ğŸ”— Merging...")
model = model.merge_and_unload()

print(f"ğŸ’¾ Saving Merged Model to {SAVE_PATH}...")
model.save_pretrained(SAVE_PATH)

print("ğŸ’¾ Saving Processor...")
processor = AutoProcessor.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
processor.save_pretrained(SAVE_PATH)

print("âœ… Done! Full model is ready.")
