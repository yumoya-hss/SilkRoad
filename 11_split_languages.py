import json
import os
import argparse
from collections import defaultdict
from tqdm import tqdm

# ==========================================
# ğŸ› ï¸ é»˜è®¤é…ç½®
# ==========================================
DEFAULT_INPUT_FILE = "dataset_optimal_filtered.json"
DEFAULT_OUTPUT_DIR = "final_datasets_split"

def load_data(file_path):
    print(f"ğŸ“– æ­£åœ¨è¯»å–æ•°æ®é›†: {file_path} ...")
    if not os.path.exists(file_path):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {file_path}")
        exit(1)
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"âŒ è¯»å–å¤±è´¥: {e}")
        exit(1)

def split_by_language_and_length(data, output_dir):
    # 1. åˆ›å»ºè¾“å‡ºç›®å½•
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"ğŸ“‚ å·²åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")

    # 2. åˆå§‹åŒ–ç¼“å­˜å®¹å™¨
    # ç»“æ„: buffers[lang]['short'] = [], buffers[lang]['long'] = []
    lang_buffers = defaultdict(lambda: {'short': [], 'long': []})
    
    print("ğŸš€ æ­£åœ¨æ‰§è¡Œ [è¯­è¨€ + é•¿çŸ­å¥] äºŒçº§æ‹†åˆ†...")
    
    # 3. éå†æ•°æ®
    stats = defaultdict(int)

    for item in tqdm(data):
        if 'translations' not in item: continue
        
        # åŸºç¡€å…ƒæ•°æ® (å›¾ç‰‡è·¯å¾„ç­‰)
        base_meta = {
            "image_id": item.get("image_id"),
            "path": item.get("path"),
            # åŸæœ‰çš„ wnid, width ç­‰ä¹Ÿå¯ä»¥å¸¦ä¸Šï¼ŒæŒ‰éœ€
        }

        # éå†è¯¥æ¡ç›®ä¸‹çš„æ‰€æœ‰è¯­è¨€
        for lang, trans_content in item['translations'].items():
            if not trans_content: continue

            # --- å¤„ç† Short Caption ---
            # åªæœ‰å½“ short_translation å­˜åœ¨ä¸”ä¸ä¸ºç©ºæ—¶æ‰ä¿å­˜
            if trans_content.get("short_translation"):
                short_entry = base_meta.copy()
                short_entry.update({
                    "type": "short",
                    "src_text": item.get("src_short"),       # ç»Ÿä¸€å­—æ®µåï¼šæºæ–‡æœ¬
                    "tgt_text": trans_content["short_translation"], # ç»Ÿä¸€å­—æ®µåï¼šç›®æ ‡æ–‡æœ¬
                    "model": trans_content.get("short_model"),
                    "scores": trans_content.get("short_scores")
                })
                lang_buffers[lang]['short'].append(short_entry)
                stats[f"{lang}_short"] += 1

            # --- å¤„ç† Long Caption ---
            # åªæœ‰å½“ long_translation å­˜åœ¨ä¸”ä¸ä¸ºç©ºæ—¶æ‰ä¿å­˜
            if trans_content.get("long_translation"):
                long_entry = base_meta.copy()
                long_entry.update({
                    "type": "long",
                    "src_text": item.get("src_long"),        # ç»Ÿä¸€å­—æ®µåï¼šæºæ–‡æœ¬
                    "tgt_text": trans_content["long_translation"],  # ç»Ÿä¸€å­—æ®µåï¼šç›®æ ‡æ–‡æœ¬
                    "model": trans_content.get("long_model"),
                    "scores": trans_content.get("long_scores")
                })
                lang_buffers[lang]['long'].append(long_entry)
                stats[f"{lang}_long"] += 1

    # 4. ä¿å­˜æ–‡ä»¶
    print("\nğŸ’¾ ä¿å­˜ç»“æœç»Ÿè®¡:")
    print("=" * 65)
    print(f"{'Language':<10} | {'Type':<6} | {'Count':<8} | {'Output Filename'}")
    print("-" * 65)

    if not lang_buffers:
        print("âš ï¸ è­¦å‘Š: æ²¡æœ‰æå–åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼")
        return

    for lang, types in lang_buffers.items():
        # ä¿å­˜ Short æ–‡ä»¶
        if types['short']:
            filename_s = f"{lang}_short.json"
            path_s = os.path.join(output_dir, filename_s)
            with open(path_s, 'w', encoding='utf-8') as f:
                json.dump(types['short'], f, ensure_ascii=False, indent=2)
            print(f"{lang:<10} | {'Short':<6} | {len(types['short']):<8} | {filename_s}")

        # ä¿å­˜ Long æ–‡ä»¶
        if types['long']:
            filename_l = f"{lang}_long.json"
            path_l = os.path.join(output_dir, filename_l)
            with open(path_l, 'w', encoding='utf-8') as f:
                json.dump(types['long'], f, ensure_ascii=False, indent=2)
            print(f"{lang:<10} | {'Long':<6} | {len(types['long']):<8} | {filename_l}")

    print("=" * 65)
    print(f"ğŸ‰ æ‹†åˆ†å®Œæˆï¼æ–‡ä»¶ä½äº '{output_dir}/' ç›®å½•ä¸‹ã€‚")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Split dataset into Language + Short/Long pairs.")
    parser.add_argument("--input_file", type=str, default=DEFAULT_INPUT_FILE)
    parser.add_argument("--output_dir", type=str, default=DEFAULT_OUTPUT_DIR)
    args = parser.parse_args()

    dataset = load_data(args.input_file)
    split_by_language_and_length(dataset, args.output_dir)